{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3dcc529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-27 11:40:30.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mPROJ_ROOT path is: D:\\CML\\Term 8\\ML projects\\forecasting_workspace\\oceanwave_forecast\u001b[0m\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ocean Wave Height and Period Forecasting with DeepAR\n",
    "# Deep Autoregressive Time Series Modeling using PyTorch Forecasting\n",
    "\n",
    "import warnings, numpy as np, pandas as pd, torch\n",
    "import matplotlib.pyplot as plt\n",
    "import lightning as pl\n",
    "import pytorch_forecasting as ptf\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from sktime.split import temporal_train_test_split\n",
    "import importlib\n",
    "\n",
    "from oceanwave_forecast import data_manager, data_pipeline, forecasting_utils, config, mlflow_utils, training\n",
    "\n",
    "importlib.reload(data_manager)\n",
    "importlib.reload(data_pipeline)\n",
    "importlib.reload(forecasting_utils)\n",
    "importlib.reload(config)\n",
    "importlib.reload(mlflow_utils)\n",
    "importlib.reload(training)\n",
    "\n",
    "from collections import namedtuple\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer, MultiNormalizer\n",
    "from sktime.transformations.series.summarize import WindowSummarizer\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "pl.seed_everything(config.RANDOM_STATE)\n",
    "torch.manual_seed(config.RANDOM_STATE)\n",
    "np.random.seed(config.RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fab36c",
   "metadata": {},
   "source": [
    "# 1. DATA PREPARATION AND PREPROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94333a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureConfig = namedtuple(\n",
    "    \"FeatureConfig\",\n",
    "    [\n",
    "        \"target\",\n",
    "        \"index_cols\",\n",
    "        \"static_categoricals\",\n",
    "        \"static_reals\",\n",
    "        \"time_varying_known_categoricals\",\n",
    "        \"time_varying_known_reals\",\n",
    "        \"time_varying_unknown_reals\",\n",
    "        \"group_ids\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "feat_cfg = FeatureConfig(\n",
    "    target              = \"Hs\",                              # <- main forecast target\n",
    "    index_cols          = [\"series\", \"timestamp\"],           # timestamp + series ID\n",
    "    static_categoricals = [\"series\"],                        # ocean buoy ID\n",
    "    static_reals        = [],\n",
    "    time_varying_known_categoricals = [],                    # e.g. holiday flags\n",
    "    time_varying_known_reals        = [\"time_idx\"],          # we always know time\n",
    "    time_varying_unknown_reals      = [],                    # filled later (lags & exog)\n",
    "    group_ids           = [\"series\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce99db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\CML\\Term 8\\ML projects\\forecasting_workspace\\oceanwave_forecast\\oceanwave_forecast\\data_manager.py:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(\n",
      "D:\\CML\\Term 8\\ML projects\\forecasting_workspace\\oceanwave_forecast\\oceanwave_forecast\\data_pipeline.py:111: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  data_ocean_hourly = data_ocean_clean.resample('H').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (52650, 13)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 52650 entries, 2024-01-01 00:00:00 to 2024-12-31 23:50:00\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   WDIR    52650 non-null  float64\n",
      " 1   WSPD    52650 non-null  float64\n",
      " 2   GST     52650 non-null  float64\n",
      " 3   WVHT    52650 non-null  float64\n",
      " 4   DPD     52650 non-null  float64\n",
      " 5   APD     52650 non-null  float64\n",
      " 6   MWD     52650 non-null  float64\n",
      " 7   PRES    52650 non-null  float64\n",
      " 8   ATMP    52650 non-null  float64\n",
      " 9   WTMP    52650 non-null  float64\n",
      " 10  DEWP    52650 non-null  float64\n",
      " 11  VIS     52650 non-null  float64\n",
      " 12  TIDE    52650 non-null  float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 5.6 MB\n",
      "\n",
      "Descriptive statistics:\n",
      "               WDIR          WSPD           GST          WVHT           DPD  \\\n",
      "count  52650.000000  52650.000000  52650.000000  52650.000000  52650.000000   \n",
      "mean     194.421026      4.962283      6.241216     66.211796     80.537227   \n",
      "std       96.600677      3.805890      4.484900     46.447783     37.382029   \n",
      "min        1.000000      0.000000      0.000000      0.030000      2.060000   \n",
      "25%      119.000000      2.300000      3.000000      0.550000     99.000000   \n",
      "50%      225.000000      4.300000      5.300000     99.000000     99.000000   \n",
      "75%      260.000000      6.900000      8.500000     99.000000     99.000000   \n",
      "max      999.000000     99.000000     99.000000     99.000000     99.000000   \n",
      "\n",
      "                APD           MWD          PRES          ATMP          WTMP  \\\n",
      "count  52650.000000  52650.000000  52650.000000  52650.000000  52650.000000   \n",
      "mean      67.337013    844.439943   1016.577221     10.072936     14.923626   \n",
      "std       44.857096    314.352896     78.656110     21.306571     71.185836   \n",
      "min        2.270000      0.000000    985.000000     -9.800000      8.000000   \n",
      "25%        4.230000    999.000000   1012.000000      8.100000      8.900000   \n",
      "50%       99.000000    999.000000   1016.600000      9.700000      9.600000   \n",
      "75%       99.000000    999.000000   1020.900000     11.500000     10.600000   \n",
      "max       99.000000    999.000000   9999.000000    999.000000    999.000000   \n",
      "\n",
      "               DEWP      VIS     TIDE  \n",
      "count  52650.000000  52650.0  52650.0  \n",
      "mean       7.827449     99.0     99.0  \n",
      "std       21.475054      0.0      0.0  \n",
      "min      -16.100000     99.0     99.0  \n",
      "25%        5.300000     99.0     99.0  \n",
      "50%        7.800000     99.0     99.0  \n",
      "75%       10.100000     99.0     99.0  \n",
      "max      999.000000     99.0     99.0  \n"
     ]
    }
   ],
   "source": [
    "raw_path   = config.RAW_DATA_DIR / \"Standard meteorological data 2024\" / \"46088h2024.txt\"\n",
    "df_raw     = data_manager.extract_raw_data(raw_path)\n",
    "df_clean   = data_pipeline.preprocess_ocean_data(df_raw)\n",
    "# df_clean   = df_clean.loc[config.START_DATE : config.END_DATE]\n",
    "\n",
    "# split target & features\n",
    "Y = df_clean[config.TARGETS]\n",
    "X = df_clean.drop(columns=config.TARGETS)\n",
    "\n",
    "y_train, y_test, X_train, X_test = temporal_train_test_split(\n",
    "    y=Y, X=X, test_size=config.HORIZON * 3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102ba87",
   "metadata": {},
   "source": [
    "# 2. FEATURE ENGINEERING FOR DEEPAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5677f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\pmdarima\\arima\\_validation.py:76: UserWarning: stepwise model cannot be fit in parallel (n_jobs=1). Falling back to stepwise parameter search.\n",
      "  warnings.warn('stepwise model cannot be fit in parallel (n_jobs=%i). '\n",
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\pmdarima\\arima\\_validation.py:76: UserWarning: stepwise model cannot be fit in parallel (n_jobs=1). Falling back to stepwise parameter search.\n",
      "  warnings.warn('stepwise model cannot be fit in parallel (n_jobs=%i). '\n",
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\pmdarima\\arima\\_validation.py:76: UserWarning: stepwise model cannot be fit in parallel (n_jobs=1). Falling back to stepwise parameter search.\n",
      "  warnings.warn('stepwise model cannot be fit in parallel (n_jobs=%i). '\n"
     ]
    }
   ],
   "source": [
    "pipe_X, pipe_Y = data_pipeline.get_pipelines(list(X_train.columns))\n",
    "\n",
    "X_train_transformed = pipe_X.fit_transform(X_train)\n",
    "X_test_transformed  = pipe_X.transform(X_test)\n",
    "y_train_transformed = pipe_Y.fit_transform(y_train)\n",
    "y_test_transformed  = pipe_Y.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57ba8718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN head:\n",
      "        WSPD       GST      PRES      ATMP      WTMP      DEWP  WDIR_sin  \\\n",
      "0 -0.936030 -0.871019  0.941417 -0.730903 -0.561287 -0.547499  1.629398   \n",
      "1 -1.191520 -1.137846  0.982225 -0.777873 -0.591542 -0.547499  1.551162   \n",
      "2 -1.415727 -1.315730  1.007164 -0.824843 -0.621797 -0.547499  0.064047   \n",
      "\n",
      "   WDIR_cos   MWD_sin   MWD_cos      WVHT       APD  series  month  hour  \\\n",
      "0  0.028270  1.912724  1.486769 -0.757493  0.779594  series      1     0   \n",
      "1 -0.006236  1.912724  1.486769 -0.698073  1.456019  series      1     1   \n",
      "2  0.731467  1.912733  1.486769 -0.772348  1.486535  series      1     2   \n",
      "\n",
      "     hr_sin    hr_cos  time_idx  \n",
      "0  0.000000  1.000000         0  \n",
      "1  0.258819  0.965926         1  \n",
      "2  0.500000  0.866025         2  \n",
      "TEST  head:\n",
      "        WSPD       GST      PRES      ATMP      WTMP      DEWP  WDIR_sin  \\\n",
      "0  2.400993  2.669726 -1.391476 -0.038090 -0.909218 -0.022984  1.488394   \n",
      "1  2.161144  2.186897 -1.389209 -0.026348 -0.909218 -0.009301  1.443436   \n",
      "2  1.895225  1.920071 -1.341599 -0.143774 -0.909218  0.086480  1.465895   \n",
      "\n",
      "   WDIR_cos   MWD_sin   MWD_cos      WVHT       APD  series  month  hour  \\\n",
      "0 -0.572391  1.800054 -0.742939  3.580141  0.708392  series     12     0   \n",
      "1 -0.677474  1.763282 -0.856619  3.208768  0.545643  series     12     1   \n",
      "2 -0.630607  1.762940 -0.856182  2.837395  0.352379  series     12     2   \n",
      "\n",
      "     hr_sin    hr_cos  time_idx  \n",
      "0  0.000000  1.000000         0  \n",
      "1  0.258819  0.965926         1  \n",
      "2  0.500000  0.866025         2  \n",
      "TRAIN shape:\n",
      " (8568, 18)\n",
      "TEST  shape:\n",
      " (216, 18)\n"
     ]
    }
   ],
   "source": [
    "def _add_calendar(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    hr = df.index.hour\n",
    "    df[\"month\"] = df.index.month\n",
    "    df[\"hour\"] = hr\n",
    "    df[\"hr_sin\"] = np.sin(2 * np.pi * hr / 24)\n",
    "    df[\"hr_cos\"] = np.cos(2 * np.pi * hr / 24)\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_long(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    series_col: str = \"series\",                #  identifier of each buoy / station\n",
    "    time_col:  str = \"timestamp\"               #  DatetimeIndex will be copied here\n",
    ") -> pd.DataFrame:\n",
    "    # combine exogenous & targets side‑by‑side\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "\n",
    "    df[time_col]   = df.index                  # DatetimeIndex → column\n",
    "    df[series_col] = X.index.get_level_values(series_col) if isinstance(\n",
    "        X.index, pd.MultiIndex\n",
    "    ) else series_col                          # constant string if only one series\n",
    "\n",
    "    # add calendar features\n",
    "    df = _add_calendar(df)\n",
    "\n",
    "    # Adding the time_idx (0,1,2,…) **within each group**\n",
    "    df[\"time_idx\"] = (\n",
    "        df.groupby(series_col)[time_col]\n",
    "          .rank(method=\"first\")\n",
    "          .astype(\"int64\") - 1\n",
    "    )\n",
    "    df = df.drop(columns=[time_col])  # drop the original DatetimeIndex\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_long = make_long(X_train_transformed, y_train_transformed)\n",
    "test_long  = make_long(X_test_transformed,  y_test_transformed)\n",
    "\n",
    "print(\"TRAIN head:\\n\", train_long.head(3))\n",
    "print(\"TEST  head:\\n\", test_long.head(3))\n",
    "print(\"TRAIN shape:\\n\", train_long.shape)\n",
    "print(\"TEST  shape:\\n\", test_long.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe5e327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WVHT', 'APD']\n"
     ]
    }
   ],
   "source": [
    "print(config.TARGETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1201bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_window_summarizer(\n",
    "    df,\n",
    "    summarizer: WindowSummarizer,\n",
    "    target_cols: list[str],\n",
    "    fit: bool = True\n",
    "):\n",
    "    # Get the target columns from the DataFrame\n",
    "    df_targets = df[target_cols]\n",
    "    \n",
    "    # Apply the summarizer\n",
    "    if fit:\n",
    "        df_lagged = summarizer.fit_transform(df_targets)\n",
    "    else:\n",
    "        df_lagged = summarizer.transform(df_targets)\n",
    "    \n",
    "    # Re-join the new features\n",
    "    return df.join(df_lagged)\n",
    "\n",
    "\n",
    "\n",
    "# Configure window summarizer\n",
    "summarizer = WindowSummarizer(\n",
    "    lag_feature=config.TARGET_WINDOWSUMMARY_CONFIG,\n",
    "    target_cols=config.TARGETS,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "train_long = apply_window_summarizer(\n",
    "    train_long,\n",
    "    summarizer,\n",
    "    config.TARGETS,\n",
    "    fit=True\n",
    ")\n",
    "\n",
    "test_long = apply_window_summarizer(\n",
    "    test_long,\n",
    "    summarizer,\n",
    "    config.TARGETS,\n",
    "    fit=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "897331f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Data Type\n",
      "WSPD              float64\n",
      "GST               float64\n",
      "PRES              float64\n",
      "ATMP              float64\n",
      "WTMP              float64\n",
      "DEWP              float64\n",
      "WDIR_sin          float64\n",
      "WDIR_cos          float64\n",
      "MWD_sin           float64\n",
      "MWD_cos           float64\n",
      "WVHT              float64\n",
      "APD               float64\n",
      "series             object\n",
      "month               int32\n",
      "hour                int32\n",
      "hr_sin            float64\n",
      "hr_cos            float64\n",
      "time_idx            int64\n",
      "WVHT_lag_1        float64\n",
      "WVHT_lag_2        float64\n",
      "WVHT_lag_3        float64\n",
      "WVHT_lag_4        float64\n",
      "WVHT_lag_24       float64\n",
      "WVHT_lag_48       float64\n",
      "WVHT_lag_72       float64\n",
      "WVHT_mean_1_24    float64\n",
      "WVHT_mean_24_48   float64\n",
      "APD_lag_1         float64\n",
      "APD_lag_2         float64\n",
      "APD_lag_3         float64\n",
      "APD_lag_4         float64\n",
      "APD_lag_24        float64\n",
      "APD_lag_48        float64\n",
      "APD_lag_72        float64\n",
      "APD_mean_1_24     float64\n",
      "APD_mean_24_48    float64\n"
     ]
    }
   ],
   "source": [
    "print(train_long.dtypes.to_frame(name=\"Data Type\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe447e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5966bb2",
   "metadata": {},
   "source": [
    "# 3. TIMESERIESDATASET CONFIGURATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bb503af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All splits valid! Proceeding with dataset creation...\n",
      "Features (time_varying_unknown_reals): ['WSPD', 'GST', 'PRES', 'ATMP', 'WTMP', 'DEWP', 'WDIR_sin', 'WDIR_cos', 'MWD_sin', 'MWD_cos']\n",
      "Targets: ['WVHT', 'APD']\n",
      "\n",
      "Creating training dataset...\n",
      "✅ Training dataset created: 7703 sequences\n",
      "Creating validation dataset...\n",
      "✅ Validation dataset created: 647 sequences\n",
      "Creating test dataset...\n",
      "✅ Test dataset created: 1 sequences\n",
      "\n",
      "🎉 All datasets created successfully!\n",
      "Summary:\n",
      "  - Training sequences: 7703\n",
      "  - Validation sequences: 647\n",
      "  - Test sequences: 1\n"
     ]
    }
   ],
   "source": [
    "# If we have valid splits, proceed with dataset creation\n",
    "if all([train_valid, val_valid, test_valid]):\n",
    "        print(f\"\\n✅ All splits valid! Proceeding with dataset creation...\")\n",
    "        \n",
    "        # Ensure target columns are properly configured\n",
    "        target_cols = config.TARGETS if isinstance(config.TARGETS, list) else [config.TARGETS]\n",
    "        feature_cols = [c for c in X_train_transformed.columns]\n",
    "        \n",
    "        # All features go into time_varying_unknown_reals\n",
    "        # Targets are automatically handled by PyTorch Forecasting\n",
    "        time_varying_unknown_reals = feature_cols\n",
    "        \n",
    "        print(f\"Features (time_varying_unknown_reals): {time_varying_unknown_reals}\")\n",
    "        print(f\"Targets: {target_cols}\")\n",
    "        \n",
    "        # build one GroupNormalizer *per* target\n",
    "        normalizers = [\n",
    "            GroupNormalizer(\n",
    "                groups=[\"series\"],\n",
    "                transformation=\"softplus\"\n",
    "            )\n",
    "            for _ in target_cols\n",
    "        ]\n",
    "\n",
    "        common = dict(\n",
    "            time_idx                   = \"time_idx\",\n",
    "            target                     = config.TARGETS,\n",
    "            group_ids                  = [\"series\"],\n",
    "            time_varying_known_reals   = [\"time_idx\"],  # Only time_idx is known in future\n",
    "            time_varying_unknown_reals = time_varying_unknown_reals,  # Features only\n",
    "            static_categoricals        = [\"series\"],\n",
    "            max_encoder_length         = enc_len,\n",
    "            max_prediction_length      = max_pred_len,\n",
    "            min_encoder_length         = max(enc_len // 2, 1),  # More flexible minimum\n",
    "            min_prediction_length      = 1,\n",
    "            target_normalizer          = MultiNormalizer(normalizers),\n",
    "            allow_missing_timesteps    = True,\n",
    "        )\n",
    "    \n",
    "    \n",
    "        print(\"\\nCreating training dataset...\")\n",
    "        train_ds = TimeSeriesDataSet(train_df, **common)\n",
    "        print(f\"✅ Training dataset created: {len(train_ds)} sequences\")\n",
    "        \n",
    "        print(\"Creating validation dataset...\")\n",
    "        val_ds = TimeSeriesDataSet.from_dataset(train_ds, val_df, stop_randomization=True)\n",
    "        print(f\"✅ Validation dataset created: {len(val_ds)} sequences\")\n",
    "        \n",
    "        print(\"Creating test dataset...\")\n",
    "        test_ds = TimeSeriesDataSet.from_dataset(\n",
    "            train_ds, test_df,\n",
    "            predict=True, stop_randomization=True\n",
    "        )\n",
    "        print(f\"✅ Test dataset created: {len(test_ds)} sequences\")\n",
    "        \n",
    "        print(f\"\\n🎉 All datasets created successfully!\")\n",
    "        print(f\"Summary:\")\n",
    "        print(f\"  - Training sequences: {len(train_ds)}\")\n",
    "        print(f\"  - Validation sequences: {len(val_ds)}\")\n",
    "        print(f\"  - Test sequences: {len(test_ds)}\")\n",
    "        \n",
    "        \n",
    " \n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot create datasets - invalid splits!\")\n",
    "    print(\"Consider:\")\n",
    "    print(\"1. Reducing WINDOW (encoder length) - try ONE_WEEK * 2 instead of 3\")  \n",
    "    print(\"2. Reducing HORIZON (prediction length) - try ONE_DAY * 2 instead of 3\")\n",
    "    print(\"3. Getting more training data\")\n",
    "    \n",
    "    # Show what would work\n",
    "    max_possible_enc = (total_length - 2 * max_pred_len) // 2\n",
    "    print(f\"4. Maximum encoder length that would work: {max_possible_enc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc2d92",
   "metadata": {},
   "source": [
    "# 4. TRAINING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfb2db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch   = config.DEEPAR_CONFIG[\"batch_size\"]\n",
    "\n",
    "train_loader = train_ds.to_dataloader(train=True,  batch_size=batch, num_workers=4)\n",
    "val_loader   = val_ds  .to_dataloader(train=False, batch_size=batch)\n",
    "test_loader  = test_ds .to_dataloader(train=False, batch_size=batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fb17aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_cat: shape torch.Size([64, 504, 1])\n",
      "encoder_cont: shape torch.Size([64, 504, 12])\n",
      "encoder_target: [tensor([[ 0.5349,  0.3715,  0.2081,  ..., -0.8912, -0.9060, -0.9209],\n",
      "        [ 0.3715,  0.2081,  0.0892,  ..., -0.9060, -0.9209,  0.0000],\n",
      "        [ 0.2081,  0.0892, -0.1187,  ..., -0.9209,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.3566,  0.9954,  1.6639,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9954,  1.6639,  2.0798,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.6639,  2.0798,  1.7233,  ...,  0.0000,  0.0000,  0.0000]]), tensor([[-0.4359, -0.4614, -0.6292,  ...,  0.4134,  0.8152,  1.5577],\n",
      "        [-0.4614, -0.6292, -0.5224,  ...,  0.8152,  1.5577,  0.0000],\n",
      "        [-0.6292, -0.5224, -0.3291,  ...,  1.5577,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1766,  0.1439,  0.1998,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1439,  0.1998,  0.1693,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1998,  0.1693, -0.0952,  ...,  0.0000,  0.0000,  0.0000]])]\n",
      "encoder_lengths: shape torch.Size([64])\n",
      "decoder_cat: shape torch.Size([64, 72, 1])\n",
      "decoder_cont: shape torch.Size([64, 72, 12])\n",
      "decoder_target: [tensor([[-0.8763, -0.8021, -0.6238,  ...,  1.4410,  2.1244,  3.0899],\n",
      "        [-0.8763, -0.8021, -0.6238,  ...,  1.4410,  2.1244,  3.0899],\n",
      "        [-0.8763, -0.8021, -0.6238,  ...,  1.4410,  2.1244,  3.0899],\n",
      "        ...,\n",
      "        [-0.8763, -0.8021, -0.6238,  ...,  1.4410,  2.1244,  3.0899],\n",
      "        [-0.8763, -0.8021, -0.6238,  ...,  1.4410,  2.1244,  3.0899],\n",
      "        [-0.8763, -0.8021, -0.6238,  ...,  1.4410,  2.1244,  3.0899]]), tensor([[ 0.0421, -0.7462, -1.0259,  ...,  0.2303,  0.6270,  0.7796],\n",
      "        [ 0.0421, -0.7462, -1.0259,  ...,  0.2303,  0.6270,  0.7796],\n",
      "        [ 0.0421, -0.7462, -1.0259,  ...,  0.2303,  0.6270,  0.7796],\n",
      "        ...,\n",
      "        [ 0.0421, -0.7462, -1.0259,  ...,  0.2303,  0.6270,  0.7796],\n",
      "        [ 0.0421, -0.7462, -1.0259,  ...,  0.2303,  0.6270,  0.7796],\n",
      "        [ 0.0421, -0.7462, -1.0259,  ...,  0.2303,  0.6270,  0.7796]])]\n",
      "decoder_lengths: shape torch.Size([64])\n",
      "decoder_time_idx: shape torch.Size([64, 72])\n",
      "groups: shape torch.Size([64, 1])\n",
      "target_scale: [tensor([[-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216]]), tensor([[-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950]])]\n"
     ]
    }
   ],
   "source": [
    "# Get one batch from val_loader\n",
    "sample_batch = next(iter(val_loader))\n",
    "# If sample_batch is a tuple, extract the first element (assumed to be the dictionary with batch data)\n",
    "if isinstance(sample_batch, tuple):\n",
    "    sample_batch = sample_batch[0]\n",
    "\n",
    "# Print keys and shapes for tensors in the batch\n",
    "for key, value in sample_batch.items():\n",
    "    if torch.is_tensor(value):\n",
    "        print(f\"{key}: shape {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "240ea7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "\u001b[32m2025-07-21 13:55:51.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m386\u001b[0m - \u001b[1mInitialized DeepAR trainer on device: cuda\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer = training.DeepARTrainer(config.DEEPAR_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fee62b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-21 13:55:51.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m559\u001b[0m - \u001b[1mStarting synthetic training for 2000 epochs\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 1/2000 | Train Loss: 0.1515 | Val Loss: 0.1673 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 2/2000 | Train Loss: 0.1444 | Val Loss: 0.1656 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 3/2000 | Train Loss: 0.1530 | Val Loss: 0.1719 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 4/2000 | Train Loss: 0.1536 | Val Loss: 0.1704 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 5/2000 | Train Loss: 0.1388 | Val Loss: 0.1601 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 6/2000 | Train Loss: 0.1417 | Val Loss: 0.1749 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 7/2000 | Train Loss: 0.1485 | Val Loss: 0.1673 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 8/2000 | Train Loss: 0.1459 | Val Loss: 0.1608 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 9/2000 | Train Loss: 0.1470 | Val Loss: 0.1743 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 10/2000 | Train Loss: 0.1425 | Val Loss: 0.1705 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.114\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 11/2000 | Train Loss: 0.1508 | Val Loss: 0.1588 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 12/2000 | Train Loss: 0.1500 | Val Loss: 0.1767 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 13/2000 | Train Loss: 0.1460 | Val Loss: 0.1697 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 14/2000 | Train Loss: 0.1510 | Val Loss: 0.1653 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 15/2000 | Train Loss: 0.1473 | Val Loss: 0.1746 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 16/2000 | Train Loss: 0.1404 | Val Loss: 0.1656 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 17/2000 | Train Loss: 0.1462 | Val Loss: 0.1641 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 18/2000 | Train Loss: 0.1392 | Val Loss: 0.1600 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 19/2000 | Train Loss: 0.1480 | Val Loss: 0.1671 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 20/2000 | Train Loss: 0.1430 | Val Loss: 0.1661 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 21/2000 | Train Loss: 0.1420 | Val Loss: 0.1630 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 22/2000 | Train Loss: 0.1392 | Val Loss: 0.1578 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 23/2000 | Train Loss: 0.1483 | Val Loss: 0.1630 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 24/2000 | Train Loss: 0.1411 | Val Loss: 0.1615 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 25/2000 | Train Loss: 0.1394 | Val Loss: 0.1536 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 26/2000 | Train Loss: 0.1394 | Val Loss: 0.1524 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 27/2000 | Train Loss: 0.1435 | Val Loss: 0.1643 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 28/2000 | Train Loss: 0.1424 | Val Loss: 0.1449 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 29/2000 | Train Loss: 0.1423 | Val Loss: 0.1614 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 30/2000 | Train Loss: 0.1420 | Val Loss: 0.1570 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 31/2000 | Train Loss: 0.1502 | Val Loss: 0.1674 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 32/2000 | Train Loss: 0.1371 | Val Loss: 0.1562 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 33/2000 | Train Loss: 0.1363 | Val Loss: 0.1628 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 34/2000 | Train Loss: 0.1344 | Val Loss: 0.1561 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 35/2000 | Train Loss: 0.1413 | Val Loss: 0.1589 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 36/2000 | Train Loss: 0.1435 | Val Loss: 0.1623 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 37/2000 | Train Loss: 0.1369 | Val Loss: 0.1547 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 38/2000 | Train Loss: 0.1330 | Val Loss: 0.1632 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 39/2000 | Train Loss: 0.1327 | Val Loss: 0.1509 | LR: 5.00e-04\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 40/2000 | Train Loss: 0.1398 | Val Loss: 0.1583 | LR: 5.00e-04\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 41/2000 | Train Loss: 0.1399 | Val Loss: 0.1617 | LR: 5.00e-04\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 42/2000 | Train Loss: 0.1386 | Val Loss: 0.1598 | LR: 5.00e-04\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m584\u001b[0m - \u001b[1mEarly stopping at epoch 43\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m594\u001b[0m - \u001b[1mSynthetic training completed!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "history = trainer.train(train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bd2f46",
   "metadata": {},
   "source": [
    "# 5. MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5114c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Processing 3 blocks of size 72\n",
      "  Block 1 MeanSquaredPercentageError: 0.2306\n",
      "  Block 1 MeanAbsolutePercentageError: 0.1674\n",
      "  Block 2 MeanSquaredPercentageError: 0.1683\n",
      "  Block 2 MeanAbsolutePercentageError: 0.1358\n",
      "  Block 3 MeanSquaredPercentageError: 0.2291\n",
      "  Block 3 MeanAbsolutePercentageError: 0.1742\n",
      "\n",
      "📊 Aggregated Scores:\n",
      "  avg_MeanSquaredPercentageError: 0.2093\n",
      "  std_MeanSquaredPercentageError: 0.0290\n",
      "  avg_MeanAbsolutePercentageError: 0.1591\n",
      "  std_MeanAbsolutePercentageError: 0.0167\n",
      "✅ DeepAR predict complete (3 blocks)\n"
     ]
    }
   ],
   "source": [
    "results = trainer.predict(y_train, y_test, config.SCORERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d71ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8628746b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
