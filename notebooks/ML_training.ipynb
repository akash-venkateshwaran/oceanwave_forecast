{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3dcc529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-21 02:57:21.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: D:\\CML\\Term 8\\ML projects\\forecasting_workspace\\oceanwave_forecast\u001b[0m\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ocean Wave Height and Period Forecasting with DeepAR\n",
    "# Deep Autoregressive Time Series Modeling using PyTorch Forecasting\n",
    "\n",
    "import warnings, numpy as np, pandas as pd, torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import lightning as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "import pytorch_forecasting as ptf\n",
    "from pytorch_forecasting import TimeSeriesDataSet, DeepAR\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "from sktime.split import temporal_train_test_split\n",
    "import importlib\n",
    "\n",
    "from oceanwave_forecast import data_manager, data_pipeline, forecasting_utils, config, mlflow_utils, training\n",
    "\n",
    "importlib.reload(data_manager)\n",
    "importlib.reload(data_pipeline)\n",
    "importlib.reload(forecasting_utils)\n",
    "importlib.reload(config)\n",
    "importlib.reload(mlflow_utils)\n",
    "importlib.reload(training)\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "pl.seed_everything(config.RANDOM_STATE)\n",
    "torch.manual_seed(config.RANDOM_STATE)\n",
    "np.random.seed(config.RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fab36c",
   "metadata": {},
   "source": [
    "# 1. DATA PREPARATION AND PREPROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94333a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureConfig = namedtuple(\n",
    "    \"FeatureConfig\",\n",
    "    [\n",
    "        \"target\",\n",
    "        \"index_cols\",\n",
    "        \"static_categoricals\",\n",
    "        \"static_reals\",\n",
    "        \"time_varying_known_categoricals\",\n",
    "        \"time_varying_known_reals\",\n",
    "        \"time_varying_unknown_reals\",\n",
    "        \"group_ids\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "feat_cfg = FeatureConfig(\n",
    "    target              = \"Hs\",                              # <- main forecast target\n",
    "    index_cols          = [\"series\", \"timestamp\"],           # timestamp + series ID\n",
    "    static_categoricals = [\"series\"],                        # ocean buoy ID\n",
    "    static_reals        = [],\n",
    "    time_varying_known_categoricals = [],                    # e.g. holiday flags\n",
    "    time_varying_known_reals        = [\"time_idx\"],          # we always know time\n",
    "    time_varying_unknown_reals      = [],                    # filled later (lags & exog)\n",
    "    group_ids           = [\"series\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ce99db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (52650, 13)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 52650 entries, 2024-01-01 00:00:00 to 2024-12-31 23:50:00\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   WDIR    52650 non-null  float64\n",
      " 1   WSPD    52650 non-null  float64\n",
      " 2   GST     52650 non-null  float64\n",
      " 3   WVHT    52650 non-null  float64\n",
      " 4   DPD     52650 non-null  float64\n",
      " 5   APD     52650 non-null  float64\n",
      " 6   MWD     52650 non-null  float64\n",
      " 7   PRES    52650 non-null  float64\n",
      " 8   ATMP    52650 non-null  float64\n",
      " 9   WTMP    52650 non-null  float64\n",
      " 10  DEWP    52650 non-null  float64\n",
      " 11  VIS     52650 non-null  float64\n",
      " 12  TIDE    52650 non-null  float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 5.6 MB\n",
      "\n",
      "Descriptive statistics:\n",
      "               WDIR          WSPD           GST          WVHT           DPD  \\\n",
      "count  52650.000000  52650.000000  52650.000000  52650.000000  52650.000000   \n",
      "mean     194.421026      4.962283      6.241216     66.211796     80.537227   \n",
      "std       96.600677      3.805890      4.484900     46.447783     37.382029   \n",
      "min        1.000000      0.000000      0.000000      0.030000      2.060000   \n",
      "25%      119.000000      2.300000      3.000000      0.550000     99.000000   \n",
      "50%      225.000000      4.300000      5.300000     99.000000     99.000000   \n",
      "75%      260.000000      6.900000      8.500000     99.000000     99.000000   \n",
      "max      999.000000     99.000000     99.000000     99.000000     99.000000   \n",
      "\n",
      "                APD           MWD          PRES          ATMP          WTMP  \\\n",
      "count  52650.000000  52650.000000  52650.000000  52650.000000  52650.000000   \n",
      "mean      67.337013    844.439943   1016.577221     10.072936     14.923626   \n",
      "std       44.857096    314.352896     78.656110     21.306571     71.185836   \n",
      "min        2.270000      0.000000    985.000000     -9.800000      8.000000   \n",
      "25%        4.230000    999.000000   1012.000000      8.100000      8.900000   \n",
      "50%       99.000000    999.000000   1016.600000      9.700000      9.600000   \n",
      "75%       99.000000    999.000000   1020.900000     11.500000     10.600000   \n",
      "max       99.000000    999.000000   9999.000000    999.000000    999.000000   \n",
      "\n",
      "               DEWP      VIS     TIDE  \n",
      "count  52650.000000  52650.0  52650.0  \n",
      "mean       7.827449     99.0     99.0  \n",
      "std       21.475054      0.0      0.0  \n",
      "min      -16.100000     99.0     99.0  \n",
      "25%        5.300000     99.0     99.0  \n",
      "50%        7.800000     99.0     99.0  \n",
      "75%       10.100000     99.0     99.0  \n",
      "max      999.000000     99.0     99.0  \n"
     ]
    }
   ],
   "source": [
    "raw_path   = config.RAW_DATA_DIR / \"Standard meteorological data 2024\" / \"46088h2024.txt\"\n",
    "df_raw     = data_manager.extract_raw_data(raw_path)\n",
    "df_clean   = data_pipeline.preprocess_ocean_data(df_raw)\n",
    "df_clean   = df_clean.loc[config.START_DATE : config.END_DATE]\n",
    "\n",
    "# split target & features\n",
    "Y = df_clean[config.TARGETS]\n",
    "X = df_clean.drop(columns=config.TARGETS)\n",
    "\n",
    "y_train, y_test, X_train, X_test = temporal_train_test_split(\n",
    "    y=Y, X=X, test_size=config.HORIZON * 3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102ba87",
   "metadata": {},
   "source": [
    "# 2. FEATURE ENGINEERING FOR DEEPAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5677f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_X, pipe_Y = data_pipeline.get_pipelines(list(X_train.columns))\n",
    "\n",
    "X_train_transformed = pipe_X.fit_transform(X_train)\n",
    "X_test_transformed  = pipe_X.transform(X_test)\n",
    "y_train_transformed = pipe_Y.fit_transform(y_train)\n",
    "y_test_transformed  = pipe_Y.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62e41826",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _df in (X_train_transformed, X_test_transformed, y_train_transformed):\n",
    "    _df[\"series\"] = \"buoy_46088\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57ba8718",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'timestamp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m Xy_train \u001b[38;5;241m=\u001b[39m X_train_transformed\u001b[38;5;241m.\u001b[39mjoin(y_train_transformed)\n\u001b[0;32m     20\u001b[0m Xy_test  \u001b[38;5;241m=\u001b[39m X_test_transformed\u001b[38;5;241m.\u001b[39mjoin(y_test_transformed)\n\u001b[1;32m---> 21\u001b[0m train_df, base_test_df \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_time_idx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mXy_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mXy_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 5\u001b[0m, in \u001b[0;36mbuild_time_idx\u001b[1;34m(train_df, test_df, ts_col, id_col)\u001b[0m\n\u001b[0;32m      3\u001b[0m test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      4\u001b[0m data  \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([train_df, test_df], sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_idx_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mts_col\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m)           \u001b[38;5;66;03m# ns since epoch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m diff  \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_idx_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_idx_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_min_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(id_col)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_idx_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'timestamp'"
     ]
    }
   ],
   "source": [
    "def build_time_idx(train_df, test_df, ts_col=\"timestamp\", id_col=\"series\"):\n",
    "    train_df[\"train\"] = True\n",
    "    test_df[\"train\"]  = False\n",
    "    data  = pd.concat([train_df, test_df], sort=False)\n",
    "    data[\"time_idx_raw\"] = data[ts_col].astype(\"int64\")           # ns since epoch\n",
    "    diff  = data[\"time_idx_raw\"].iloc[1] - data[\"time_idx_raw\"].iloc[0]\n",
    "    data[\"_min_idx\"] = data.groupby(id_col)[\"time_idx_raw\"].transform(\"min\")\n",
    "    data[\"time_idx\"] = ((data[\"time_idx_raw\"] - data[\"_min_idx\"]) / diff).astype(int)\n",
    "    data.drop(columns=[\"_min_idx\", \"time_idx_raw\"], inplace=True)\n",
    "    train_df = data.loc[data.train].drop(columns=\"train\")\n",
    "    test_df  = data.loc[~data.train].drop(columns=\"train\")\n",
    "    return train_df, test_df\n",
    "\n",
    "# Remove the duplicate 'series' column from y before joining\n",
    "y_train_transformed = y_train_transformed.drop(columns=\"series\")\n",
    "\n",
    "\n",
    "\n",
    "Xy_train = X_train_transformed.join(y_train_transformed)\n",
    "Xy_test  = X_test_transformed.join(y_test_transformed)\n",
    "train_df, base_test_df = build_time_idx(\n",
    "    Xy_train.reset_index().rename(columns={\"index\": \"timestamp\"}),\n",
    "    Xy_test.reset_index(). rename(columns={\"index\": \"timestamp\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_hours = 48 * 2\n",
    "val_hours     = 48\n",
    "\n",
    "cutoff_ts   = train_df.timestamp.max() - pd.Timedelta(val_hours/48, \"D\")\n",
    "hist_cutoff = train_df.timestamp.max() - pd.Timedelta((history_hours+val_hours)/48, \"D\")\n",
    "\n",
    "val_history = train_df[(train_df.timestamp >= hist_cutoff) & (train_df.timestamp <= cutoff_ts)]\n",
    "val_df      = train_df[train_df.timestamp >  cutoff_ts]\n",
    "train_df    = train_df[train_df.timestamp <= cutoff_ts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5966bb2",
   "metadata": {},
   "source": [
    "# 3. TIMESERIESDATASET CONFIGURATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb503af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training cutoff\n",
    "static_cat   = [\"series\", \"tgt_name\"]\n",
    "known_reals  = [\"time_idx\"]                       # always known into future\n",
    "unknown_reals = X_train_transformed.columns.tolist()           # your exog lags & calendar feats\n",
    "\n",
    "max_enc_len  = config.WINDOW             # history\n",
    "max_pred_len = config.HORIZON                     # horizon\n",
    "\n",
    "train_ds = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"target\",\n",
    "    group_ids=static_cat,\n",
    "    static_categoricals=static_cat,\n",
    "    time_varying_known_reals=known_reals,\n",
    "    time_varying_unknown_reals=unknown_reals,\n",
    "    max_encoder_length=max_enc_len,\n",
    "    max_prediction_length=max_pred_len,\n",
    "    min_encoder_length=24,\n",
    "    target_normalizer=ptf.GroupNormalizer(groups=static_cat, transformation=\"softplus\"),\n",
    "    allow_missing_timesteps=True,\n",
    ")\n",
    "\n",
    "val_ds = TimeSeriesDataSet.from_dataset(train_ds, test_df, predict=True, stop_randomization=True)\n",
    "\n",
    "train_loader = train_ds.to_dataloader(train=True,  batch_size=64, num_workers=4)\n",
    "val_loader   = val_ds.to_dataloader(  train=False, batch_size=64, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc2d92",
   "metadata": {},
   "source": [
    "# 4. CREATE TIMESERIESDATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb2db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare categorical encoders\n",
    "categorical_encoders = {}\n",
    "for cat_col in static_categoricals + time_varying_known_categoricals:\n",
    "    if cat_col in deepar_data.columns:\n",
    "        categorical_encoders[cat_col] = NaNLabelEncoder().fit(deepar_data[cat_col].astype(str))\n",
    "\n",
    "# Create training dataset\n",
    "training = TimeSeriesDataSet(\n",
    "    deepar_data[deepar_data.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=config.TARGETS[0],  # Primary target (e.g., wave height)\n",
    "    group_ids=[\"series\"],\n",
    "    \n",
    "    # Categorical features\n",
    "    static_categoricals=static_categoricals,\n",
    "    time_varying_known_categoricals=time_varying_known_categoricals,\n",
    "    categorical_encoders=categorical_encoders,\n",
    "    \n",
    "    # Continuous features  \n",
    "    time_varying_known_reals=time_varying_known_reals,\n",
    "    time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "    \n",
    "    # Sequence lengths\n",
    "    max_encoder_length=MAX_ENCODER_LENGTH,\n",
    "    max_prediction_length=MAX_PREDICTION_LENGTH,\n",
    "    min_encoder_length=MIN_ENCODER_LENGTH,\n",
    "    min_prediction_length=MIN_PREDICTION_LENGTH,\n",
    "    \n",
    "    # Normalization - using GroupNormalizer for better stability\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"series\"], \n",
    "        transformation=\"softplus\",\n",
    "        center=True\n",
    "    ),\n",
    "    \n",
    "    # Handle missing values\n",
    "    allow_missing_timesteps=True,\n",
    "    \n",
    "    # Add encoder length as feature (helps model understand sequence position)\n",
    "    add_encoder_length=True,\n",
    "    \n",
    "    # Add relative time index \n",
    "    add_relative_time_idx=True,\n",
    "    \n",
    "    # Add target scales as features\n",
    "    add_target_scales=True,\n",
    "    \n",
    "    # Randomize length for regularization\n",
    "    randomize_length=(0.1, 0.2),\n",
    ")\n",
    "\n",
    "# Create validation dataset\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training, \n",
    "    deepar_data, \n",
    "    min_prediction_idx=training_cutoff + 1,\n",
    "    stop_randomization=True\n",
    ")\n",
    "\n",
    "print(f\"Training dataset: {len(training)} samples\")\n",
    "print(f\"Validation dataset: {len(validation)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ea7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee62b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
