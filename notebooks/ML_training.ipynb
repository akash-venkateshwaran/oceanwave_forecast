{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b3dcc529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-28 01:41:41.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mPROJ_ROOT path is: D:\\CML\\Term 8\\ML projects\\forecasting_workspace\\oceanwave_forecast\u001b[0m\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ocean Wave Height and Period Forecasting with DeepAR\n",
    "# Deep Autoregressive Time Series Modeling using PyTorch Forecasting\n",
    "\n",
    "import warnings, numpy as np, pandas as pd, torch\n",
    "import matplotlib.pyplot as plt\n",
    "import lightning as pl\n",
    "import pytorch_forecasting as ptf\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from sktime.split import temporal_train_test_split\n",
    "import importlib\n",
    "\n",
    "\n",
    "from oceanwave_forecast import data_manager, data_pipeline, forecasting_utils, config, mlflow_utils, training, plotting\n",
    "\n",
    "importlib.reload(data_manager)\n",
    "importlib.reload(data_pipeline)\n",
    "importlib.reload(forecasting_utils)\n",
    "importlib.reload(config)\n",
    "importlib.reload(mlflow_utils)\n",
    "importlib.reload(training)\n",
    "importlib.reload(plotting)\n",
    "\n",
    "from collections import namedtuple\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer, MultiNormalizer\n",
    "from sktime.transformations.series.summarize import WindowSummarizer\n",
    "\n",
    "from pytorch_forecasting import DeepAR\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_forecasting import MultiLoss, NormalDistributionLoss\n",
    "\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "pl.seed_everything(config.RANDOM_STATE)\n",
    "torch.manual_seed(config.RANDOM_STATE)\n",
    "np.random.seed(config.RANDOM_STATE)\n",
    "\n",
    "import mlflow\n",
    "from mlflow.exceptions import MlflowException\n",
    "from mlflow.tracking import MlflowClient\n",
    "from urllib.parse import quote\n",
    "\n",
    "\n",
    "from pytorch_forecasting.metrics import MAE, RMSE, SMAPE, MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fab36c",
   "metadata": {},
   "source": [
    "# 1. DATA PREPARATION AND PREPROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "94333a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-28 01:20:03.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfetch_and_log_runs_structured\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mMLflow tracking URI set to: file:///D:%5CCML%5CTerm%208%5CML%20projects%5Cforecasting_workspace%5Coceanwave_forecast%5Cmlruns\u001b[0m\n",
      "\u001b[32m2025-07-28 01:20:03.580\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mfetch_and_log_runs_structured\u001b[0m:\u001b[36m18\u001b[0m - \u001b[31m\u001b[1mExperiment 'Oceanwave_DeepAR_Training' not found at D:\\CML\\Term 8\\ML projects\\forecasting_workspace\\oceanwave_forecast\\mlruns.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# CHecking the experiment and its runs:\n",
    "\n",
    "def fetch_and_log_runs_structured(experiment_name: str):\n",
    "    \"\"\"Fetch and display MLflow runs organized by run_number, using PROJ_ROOT/mlruns.\"\"\"\n",
    "    # ——— 1. ensure tracking URI is set to PROJ_ROOT/mlruns ———\n",
    "    mlruns_path = config.PROJ_ROOT / \"mlruns\"\n",
    "    mlruns_path.mkdir(parents=True, exist_ok=True)\n",
    "    # URL‑encode to handle spaces, windows paths, etc.\n",
    "    uri = f\"file:///{quote(str(mlruns_path.absolute()), safe=':/')}\"\n",
    "    mlflow.set_tracking_uri(uri)\n",
    "    logger.info(f\"MLflow tracking URI set to: {uri}\")\n",
    "\n",
    "    try:\n",
    "        # ——— 2. locate experiment by name ———\n",
    "        client = MlflowClient()\n",
    "        exp = client.get_experiment_by_name(experiment_name)\n",
    "        if exp is None:\n",
    "            logger.error(f\"Experiment '{experiment_name}' not found at {mlruns_path}.\")\n",
    "            return\n",
    "\n",
    "        # ——— 3. pull runs as DataFrame ———\n",
    "        runs_df = mlflow.search_runs(\n",
    "            experiment_ids=[exp.experiment_id],\n",
    "            order_by=[\"attributes.start_time DESC\"],\n",
    "            output_format=\"pandas\",\n",
    "        )\n",
    "        if runs_df.empty:\n",
    "            logger.warning(f\"No runs found for experiment '{experiment_name}'.\")\n",
    "            return\n",
    "\n",
    "        # ——— 4. assemble detailed run info ———\n",
    "        runs_data = []\n",
    "        for rid in runs_df[\"run_id\"]:\n",
    "            run = client.get_run(rid)\n",
    "            runs_data.append({\n",
    "                \"run_id\": rid,\n",
    "                \"params\": run.data.params,\n",
    "                \"metrics\": run.data.metrics,\n",
    "            })\n",
    "        # sort by the integer run_number param\n",
    "        runs_data.sort(key=lambda x: int(x[\"params\"][\"run_number\"]))\n",
    "\n",
    "        # ——— 5. pretty‑print ———\n",
    "        print(\"=\" * 60)\n",
    "        print(\"MLFLOW EXPERIMENT RUNS\")\n",
    "        print(\"=\" * 60)\n",
    "        for run_data in runs_data:\n",
    "            params = run_data[\"params\"]\n",
    "            metrics = run_data[\"metrics\"]\n",
    "            run_number = params[\"run_number\"]\n",
    "            model_name = params.get(\"model_type\", params.get(\"model_name\", \"Unknown\"))\n",
    "\n",
    "            print(f\"\\n🔹 RUN {run_number} - {model_name}\")\n",
    "            print(\"-\" * 40)\n",
    "            print(\"📋 PARAMETERS:\")\n",
    "            for k, v in params.items():\n",
    "                if k not in {\"run_number\", \"model_type\", \"model_name\"}:\n",
    "                    print(f\"  • {k}: {v}\")\n",
    "            print(\"📊 METRICS:\")\n",
    "            if metrics:\n",
    "                for mk, mv in metrics.items():\n",
    "                    print(f\"  • {mk}: {mv:.6f}\")\n",
    "            else:\n",
    "                print(\"  • No metrics recorded\")\n",
    "            print()\n",
    "\n",
    "    except MlflowException as e:\n",
    "        logger.exception(f\"MLflow error: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Unexpected error: {e}\")\n",
    "\n",
    "fetch_and_log_runs_structured(config.MLFLOW_DEEPAR_CONFIG['experiment_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9ce99db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\CML\\Term 8\\ML projects\\forecasting_workspace\\oceanwave_forecast\\oceanwave_forecast\\data_manager.py:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (52650, 13)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 52650 entries, 2024-01-01 00:00:00 to 2024-12-31 23:50:00\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   WDIR    52650 non-null  float64\n",
      " 1   WSPD    52650 non-null  float64\n",
      " 2   GST     52650 non-null  float64\n",
      " 3   WVHT    52650 non-null  float64\n",
      " 4   DPD     52650 non-null  float64\n",
      " 5   APD     52650 non-null  float64\n",
      " 6   MWD     52650 non-null  float64\n",
      " 7   PRES    52650 non-null  float64\n",
      " 8   ATMP    52650 non-null  float64\n",
      " 9   WTMP    52650 non-null  float64\n",
      " 10  DEWP    52650 non-null  float64\n",
      " 11  VIS     52650 non-null  float64\n",
      " 12  TIDE    52650 non-null  float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 5.6 MB\n",
      "\n",
      "Descriptive statistics:\n",
      "               WDIR          WSPD           GST          WVHT           DPD  \n",
      "count  52650.000000  52650.000000  52650.000000  52650.000000  52650.000000   \n",
      "mean     194.421026      4.962283      6.241216     66.211796     80.537227   \n",
      "std       96.600677      3.805890      4.484900     46.447783     37.382029   \n",
      "min        1.000000      0.000000      0.000000      0.030000      2.060000   \n",
      "25%      119.000000      2.300000      3.000000      0.550000     99.000000   \n",
      "50%      225.000000      4.300000      5.300000     99.000000     99.000000   \n",
      "75%      260.000000      6.900000      8.500000     99.000000     99.000000   \n",
      "max      999.000000     99.000000     99.000000     99.000000     99.000000   \n",
      "\n",
      "                APD           MWD          PRES          ATMP          WTMP  \n",
      "count  52650.000000  52650.000000  52650.000000  52650.000000  52650.000000   \n",
      "mean      67.337013    844.439943   1016.577221     10.072936     14.923626   \n",
      "std       44.857096    314.352896     78.656110     21.306571     71.185836   \n",
      "min        2.270000      0.000000    985.000000     -9.800000      8.000000   \n",
      "25%        4.230000    999.000000   1012.000000      8.100000      8.900000   \n",
      "50%       99.000000    999.000000   1016.600000      9.700000      9.600000   \n",
      "75%       99.000000    999.000000   1020.900000     11.500000     10.600000   \n",
      "max       99.000000    999.000000   9999.000000    999.000000    999.000000   \n",
      "\n",
      "               DEWP      VIS     TIDE  \n",
      "count  52650.000000  52650.0  52650.0  \n",
      "mean       7.827449     99.0     99.0  \n",
      "std       21.475054      0.0      0.0  \n",
      "min      -16.100000     99.0     99.0  \n",
      "25%        5.300000     99.0     99.0  \n",
      "50%        7.800000     99.0     99.0  \n",
      "75%       10.100000     99.0     99.0  \n",
      "max      999.000000     99.0     99.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\CML\\Term 8\\ML projects\\forecasting_workspace\\oceanwave_forecast\\oceanwave_forecast\\data_pipeline.py:111: FutureWarning: 'H' is deprecated and will be removed in a future version. Use ``sep='h'`` instead.\n",
      "  data_ocean_hourly = data_ocean_clean.resample('H').mean()\n"
     ]
    }
   ],
   "source": [
    "raw_path   = config.RAW_DATA_DIR / \"Standard meteorological data 2024\" / \"46088h2024.txt\"\n",
    "df_raw     = data_manager.extract_raw_data(raw_path)\n",
    "df_clean   = data_pipeline.preprocess_ocean_data(df_raw)\n",
    "# df_clean   = df_clean.loc[config.START_DATE : config.END_DATE]\n",
    "\n",
    "# split target & features\n",
    "Y = df_clean[config.TARGETS]\n",
    "X = df_clean.drop(columns=config.TARGETS)\n",
    "\n",
    "y_train, y_test, X_train, X_test = temporal_train_test_split(\n",
    "    y=Y, X=X, test_size=config.HORIZON * 10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102ba87",
   "metadata": {},
   "source": [
    "# 2. FEATURE ENGINEERING FOR DEEPAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b5677f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\pmdarima\\arima\\_validation.py:76: UserWarning: stepwise model cannot be fit in parallel (n_jobs=1). Falling back to stepwise parameter search.\n",
      "  warnings.warn('stepwise model cannot be fit in parallel (n_jobs=%i). ')\n",
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\pmdarima\\arima\\_validation.py:76: UserWarning: stepwise model cannot be fit in parallel (n_jobs=1). Falling back to stepwise parameter search.\n",
      "  warnings.warn('stepwise model cannot be fit in parallel (n_jobs=%i). ')\n",
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\pmdarima\\arima\\_validation.py:76: UserWarning: stepwise model cannot be fit in parallel (n_jobs=1). Falling back to stepwise parameter search.\n",
      "  warnings.warn('stepwise model cannot be fit in parallel (n_jobs=%i). ')\n"
     ]
    }
   ],
   "source": [
    "pipe_X, pipe_Y = data_pipeline.get_pipelines(list(X_train.columns))\n",
    "\n",
    "X_train_transformed = pipe_X.fit_transform(X_train)\n",
    "X_test_transformed  = pipe_X.transform(X_test)\n",
    "y_train_transformed = pipe_Y.fit_transform(y_train)\n",
    "y_test_transformed  = pipe_Y.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc0119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "57ba8718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN head:\n",
      "        WSPD       GST      PRES      ATMP      WTMP      DEWP  WDIR_sin  \\n\n",
      "0 -0.945944 -0.877987  0.973289 -0.763690 -0.601870 -0.572855  1.712621   \\n\n",
      "1 -1.204516 -1.147758  1.014906 -0.810166 -0.631785 -0.572855  1.633114   \\n\n",
      "2 -1.431427 -1.327605  1.040339 -0.856642 -0.661700 -0.572855  0.121828   \\n\n",
      "\\n\n",
      "   WDIR_cos   MWD_sin   MWD_cos      WVHT       APD  group_id  month  hour  \\n\n",
      "0  0.050028  1.967362  1.453692 -0.767869  0.790348  group_id      1     0   \\n\n",
      "1  0.015358  1.967362  1.453692 -0.706999  1.472553  group_id      1     1   \\n\n",
      "2  0.756579  1.967336  1.453692 -0.783087  1.503329  group_id      1     2   \\n\n",
      "\\n\n",
      "     hr_sin    hr_cos  time_idx  \\n\n",
      "0  0.000000  1.000000         0  \\n\n",
      "1  0.258819  0.965926         1  \\n\n",
      "2  0.500000  0.866025         2  \\n\n",
      "TEST  head:\n",
      "        WSPD       GST      PRES      ATMP      WTMP      DEWP  WDIR_sin  \\n\n",
      "0 -0.170226 -0.308471  1.151319 -0.577787 -0.586912 -0.523410  0.351806   \\n\n",
      "1 -0.212442 -0.364138  1.162880 -0.548739 -0.601870 -0.523410  0.616580   \\n\n",
      "2 -0.624047 -0.702422  1.190625 -0.624262 -0.586912 -0.536895  1.048831   \\n\n",
      "\\n\n",
      "   WDIR_cos   MWD_sin   MWD_cos      WVHT       APD  group_id  month  hour  \\n\n",
      "0  1.928985 -0.577140  0.258307 -0.722217 -0.953636  group_id     12     0   \\n\n",
      "1  1.900074 -0.468906  0.258307 -0.904827  0.944229  group_id     12     1   \\n\n",
      "2  1.685907 -0.375866  0.258307 -0.920045  1.293025  group_id     12     2   \\n\n",
      "\\n\n",
      "     hr_sin    hr_cos  time_idx  \\n\n",
      "0  0.000000  1.000000         0  \\n\n",
      "1  0.258819  0.965926         1  \\n\n",
      "2  0.500000  0.866025         2  \\n\n",
      "TRAIN shape:\n",
      " (8064, 18)\n",
      "TEST  shape:\n",
      " (720, 18)\n"
     ]
    }
   ],
   "source": [
    "def _add_calendar(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    hr = df.index.hour\n",
    "    df[\"month\"] = df.index.month\n",
    "    df[\"hour\"] = hr\n",
    "    df[\"hr_sin\"] = np.sin(2 * np.pi * hr / 24)\n",
    "    df[\"hr_cos\"] = np.cos(2 * np.pi * hr / 24)\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_long(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    series_col: str = config.ID_COLS[0],                #  identifier of each buoy / station\n",
    "    time_col:  str = \"timestamp\"               #  DatetimeIndex will be copied here\n",
    ") -> pd.DataFrame:\n",
    "    # combine exogenous & targets side‑by‑side\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "\n",
    "    df[time_col]   = df.index                  # DatetimeIndex → column\n",
    "    df[series_col] = X.index.get_level_values(series_col) if isinstance(\n",
    "        X.index, pd.MultiIndex\n",
    "    ) else series_col                          # constant string if only one series\n",
    "\n",
    "    # add calendar features\n",
    "    df = _add_calendar(df)\n",
    "\n",
    "    # Adding the time_idx (0,1,2,…) **within each group**\n",
    "    df[config.ID_COLS[1]] = (\n",
    "        df.groupby(series_col)[time_col]\n",
    "          .rank(method=\"first\")\n",
    "          .astype(\"int64\") - 1\n",
    "    )\n",
    "    df = df.drop(columns=[time_col])  # drop the original DatetimeIndex\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_long = make_long(X_train_transformed, y_train_transformed)\n",
    "test_long  = make_long(X_test_transformed,  y_test_transformed)\n",
    "\n",
    "print(\"TRAIN head:\\n\", train_long.head(3))\n",
    "print(\"TEST  head:\\n\", test_long.head(3))\n",
    "print(\"TRAIN shape:\\n\", train_long.shape)\n",
    "print(\"TEST  shape:\\n\", test_long.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "db1201bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN head:\n",
      "        WSPD       GST      PRES      ATMP      WTMP      DEWP  WDIR_sin  \\n\n",
      "0 -0.945944 -0.877987  0.973289 -0.763690 -0.601870 -0.572855  1.712621   \\n\n",
      "1 -1.204516 -1.147758  1.014906 -0.810166 -0.631785 -0.572855  1.633114   \\n\n",
      "2 -1.431427 -1.327605  1.040339 -0.856642 -0.661700 -0.572855  0.121828   \\n\n",
      "\\n\n",
      "   WDIR_cos   MWD_sin   MWD_cos  ...  WVHT_mean_24_48  APD_lag_1 APD_lag_2  \\n\n",
      "0  0.050028  1.967362  1.453692  ...              NaN        NaN       NaN   \\n\n",
      "1  0.015358  1.967362  1.453692  ...              NaN   0.790348       NaN   \\n\n",
      "2  0.756579  1.967336  1.453692  ...              NaN   1.472553  0.790348   \\n\n",
      "\\n\n",
      "   APD_lag_3  APD_lag_4  APD_lag_24  APD_lag_48  APD_lag_72  APD_mean_1_24  \\n\n",
      "0        NaN        NaN         NaN         NaN         NaN            NaN   \\n\n",
      "1        NaN        NaN         NaN         NaN         NaN            NaN   \\n\n",
      "2        NaN        NaN         NaN         NaN         NaN            NaN   \\n\n",
      "\\n\n",
      "   APD_mean_24_48  \\n\n",
      "0             NaN  \\n\n",
      "1             NaN  \\n\n",
      "2             NaN  \\n\n",
      "\\n\n",
      "[3 rows x 36 columns]\n",
      "TEST  head:\n",
      "        WSPD       GST      PRES      ATMP      WTMP      DEWP  WDIR_sin  \\n\n",
      "0 -0.170226 -0.308471  1.151319 -0.577787 -0.586912 -0.523410  0.351806   \\n\n",
      "1 -0.212442 -0.364138  1.162880 -0.548739 -0.601870 -0.523410  0.616580   \\n\n",
      "2 -0.624047 -0.702422  1.190625 -0.624262 -0.586912 -0.536895  1.048831   \\n\n",
      "\\n\n",
      "   WDIR_cos   MWD_sin   MWD_cos  ...  WVHT_mean_24_48  APD_lag_1 APD_lag_2  \\n\n",
      "0  1.928985 -0.577140  0.258307  ...              NaN        NaN       NaN   \\n\n",
      "1  1.900074 -0.468906  0.258307  ...              NaN  -0.953636       NaN   \\n\n",
      "2  1.685907 -0.375866  0.258307  ...              NaN   0.944229 -0.953636   \\n\n",
      "\\n\n",
      "   APD_lag_3  APD_lag_4  APD_lag_24  APD_lag_48  APD_lag_72  APD_mean_1_24  \\n\n",
      "0        NaN        NaN         NaN         NaN         NaN            NaN   \\n\n",
      "1        NaN        NaN         NaN         NaN         NaN            NaN   \\n\n",
      "2        NaN        NaN         NaN         NaN         NaN            NaN   \\n\n",
      "\\n\n",
      "   APD_mean_24_48  \\n\n",
      "0             NaN  \\n\n",
      "1             NaN  \\n\n",
      "2             NaN  \\n\n",
      "\\n\n",
      "[3 rows x 36 columns]\n",
      "TRAIN shape:\n",
      " (8064, 36)\n",
      "TEST  shape:\n",
      " (720, 36)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def apply_window_summarizer(\n",
    "    df,\n",
    "    summarizer: WindowSummarizer,\n",
    "    target_cols: list[str],\n",
    "    fit: bool = True\n",
    "):\n",
    "    # Get the target columns from the DataFrame\n",
    "    df_targets = df[target_cols]\n",
    "    \n",
    "    # Apply the summarizer\n",
    "    if fit:\n",
    "        df_lagged = summarizer.fit_transform(df_targets)\n",
    "    else:\n",
    "        df_lagged = summarizer.transform(df_targets)\n",
    "    \n",
    "    # Re-join the new features\n",
    "    return df.join(df_lagged)\n",
    "\n",
    "\n",
    "\n",
    "# Configure window summarizer\n",
    "summarizer = WindowSummarizer(\n",
    "    lag_feature=config.TARGET_WINDOWSUMMARY_CONFIG,\n",
    "    target_cols=config.TARGETS,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "train_long = apply_window_summarizer(\n",
    "    train_long,\n",
    "    summarizer,\n",
    "    config.TARGETS,\n",
    "    fit=True\n",
    ")\n",
    "\n",
    "test_long = apply_window_summarizer(\n",
    "    test_long,\n",
    "    summarizer,\n",
    "    config.TARGETS,\n",
    "    fit=False\n",
    ")\n",
    "\n",
    "print(\"TRAIN head:\\n\", train_long.head(3))\n",
    "print(\"TEST  head:\\n\", test_long.head(3))\n",
    "print(\"TRAIN shape:\\n\", train_long.shape)\n",
    "print(\"TEST  shape:\\n\", test_long.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "897331f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Data Type\n",
      "WSPD              float64\n",
      "GST               float64\n",
      "PRES              float64\n",
      "ATMP              float64\n",
      "WTMP              float64\n",
      "DEWP              float64\n",
      "WDIR_sin          float64\n",
      "WDIR_cos          float64\n",
      "MWD_sin           float64\n",
      "MWD_cos           float64\n",
      "WVHT              float64\n",
      "APD               float64\n",
      "group_id           object\n",
      "month               int32\n",
      "hour                int32\n",
      "hr_sin            float64\n",
      "hr_cos            float64\n",
      "time_idx            int64\n",
      "WVHT_lag_1        float64\n",
      "WVHT_lag_2        float64\n",
      "WVHT_lag_3        float64\n",
      "WVHT_lag_4        float64\n",
      "WVHT_lag_24       float64\n",
      "WVHT_lag_48       float64\n",
      "WVHT_lag_72       float64\n",
      "WVHT_mean_1_24    float64\n",
      "WVHT_mean_24_48   float64\n",
      "APD_lag_1         float64\n",
      "APD_lag_2         float64\n",
      "APD_lag_3         float64\n",
      "APD_lag_4         float64\n",
      "APD_lag_24        float64\n",
      "APD_lag_48        float64\n",
      "APD_lag_72        float64\n",
      "APD_mean_1_24     float64\n",
      "APD_mean_24_48    float64\n"
     ]
    }
   ],
   "source": [
    "print(train_long.dtypes.to_frame(name=\"Data Type\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "75fe447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariates: ['WSPD', 'GST', 'PRES', 'ATMP', 'WTMP', 'DEWP', 'WDIR_sin', 'WDIR_cos', 'MWD_sin', 'MWD_cos', 'month', 'hour', 'hr_sin', 'hr_cos', 'WVHT_lag_1', 'WVHT_lag_2', 'WVHT_lag_3', 'WVHT_lag_4', 'WVHT_lag_24', 'WVHT_lag_48', 'WVHT_lag_72', 'WVHT_mean_1_24', 'WVHT_mean_24_48', 'APD_lag_1', 'APD_lag_2', 'APD_lag_3', 'APD_lag_4', 'APD_lag_24', 'APD_lag_48', 'APD_lag_72', 'APD_mean_1_24', 'APD_mean_24_48']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build the exclude list\n",
    "exclude = set(config.TARGETS + config.ID_COLS)\n",
    "\n",
    "# all other columns become covariates\n",
    "covariate_variables = [col for col in train_long.columns if col not in exclude]\n",
    "\n",
    "print(\"Covariates:\", covariate_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5ba8c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_long_clean = train_long.dropna(subset=covariate_variables).reset_index(drop=True)\n",
    "test_long_clean  = test_long.dropna(subset=covariate_variables).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5966bb2",
   "metadata": {},
   "source": [
    "# 3. TIMESERIESDATASET CONFIGURATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e51988e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WVHT', 'APD']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0bb503af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. build the training TimeSeriesDataSet\n",
    "train_ds = TimeSeriesDataSet(\n",
    "    data=train_long_clean,\n",
    "    time_idx=config.ID_COLS[1],\n",
    "    target=config.TARGETS,\n",
    "    group_ids=[config.ID_COLS[0]], \n",
    "    max_encoder_length=config.WINDOW,\n",
    "    max_prediction_length=config.HORIZON,\n",
    "    static_reals=None,\n",
    "    time_varying_known_categoricals=None,\n",
    "    time_varying_known_reals=covariate_variables,\n",
    "    time_varying_unknown_categoricals=None,\n",
    "    time_varying_unknown_reals=config.TARGETS,\n",
    "    target_normalizer=MultiNormalizer(\n",
    "        [GroupNormalizer(groups=[config.ID_COLS[0]]) for _ in config.TARGETS]\n",
    "    ),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True\n",
    ")\n",
    "\n",
    "test_ds = TimeSeriesDataSet.from_dataset(\n",
    "    train_ds, \n",
    "    test_long_clean, \n",
    "    stop_randomization=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5d33c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_ds.to_dataloader(train=True, batch_size=config.BATCH_SIZE, num_workers=config.NUM_WORKERS)\n",
    "test_loader = test_ds.to_dataloader(train=False, batch_size=config.BATCH_SIZE, num_workers=config.NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "498751cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\CML\\Term 8\\ML projects\\forecasting_workspace\\oceanwave_forecast\\oceanwave_forecast\\plotting.py:112: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch data visualization saved to D:\\CML\\Term 8\\ML projects\\forecasting_workspace\\oceanwave_forecast\\reports\\testing\\Testing_data_sample.png\n"
     ]
    }
   ],
   "source": [
    "plotting.plot_dataloader_sample(train_loader, config.TESTING_REPORTS_DIR / \"Testing_data_sample.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc2d92",
   "metadata": {},
   "source": [
    "# 4. TRAINING AND EVALUATION WITH MLFLOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "new_training_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "def run_deepar_training(\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    train_ds,\n",
    "    run_number: int,\n",
    "    model_name: str = \"DeepAR\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Wraps the DeepAR training and evaluation in an MLflow experiment.\n",
    "    \"\"\"\n",
    "    exp_manager = mlflow_utils.MLflowExperimentManager(\n",
    "        experiment_name=config.MLFLOW_DEEPAR_CONFIG['experiment_name'],\n",
    "        run_number=run_number,\n",
    "        tags=config.MLFLOW_DEEPAR_CONFIG['tags']\n",
    "    )\n",
    "\n",
    "    run = exp_manager.start_mlflow_run(run_name_prefix=model_name)\n",
    "\n",
    "    if run.info.status == \"FINISHED\":\n",
    "        print(f\"{model_name} run already complete (id={run.info.run_id}) – skipping.\")\n",
    "        return run\n",
    "\n",
    "    try:\n",
    "        # Log parameters\n",
    "        exp_manager.log_params(config.DEEPAR_CONFIG)\n",
    "        exp_manager.log_param(\"model_type\", model_name)\n",
    "        exp_manager.log_param(\"targets\", \",\".join(config.TARGETS))\n",
    "        exp_manager.log_param(\"batch_size\", config.BATCH_SIZE)\n",
    "        exp_manager.log_param(\"max_epochs\", config.MAX_EPOCHS)\n",
    "        exp_manager.log_param(\"learning_rate\", config.LEARNING_RATE)\n",
    "        exp_manager.log_param(\"gradient_clip_val\", config.GRADIENT_CLIP_VAL)\n",
    "        exp_manager.log_param(\"optimizer\", config.OPTIMIZER)\n",
    "        exp_manager.log_param(\"window\", config.WINDOW)\n",
    "        exp_manager.log_param(\"horizon\", config.HORIZON)\n",
    "\n",
    "        # Setup callbacks\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=config.EARLY_STOP_PATIENCE,\n",
    "            min_delta=config.EARLY_STOP_MIN_DELTA,\n",
    "            mode=\"min\",\n",
    "        )\n",
    "        lr_logger = LearningRateMonitor(logging_interval=\"epoch\")\n",
    "        checkpoint = ModelCheckpoint(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1,\n",
    "            filename=f\"{model_name}-{{epoch:02d}}-val_loss={{val_loss:.4f}}\"\n",
    "        )\n",
    "        print_metrics = training.PrintMetricsCallback(print_every_n_epochs=10)\n",
    "\n",
    "        # Setup trainer\n",
    "        trainer = Trainer(\n",
    "            max_epochs=config.MAX_EPOCHS,\n",
    "            accelerator=\"auto\",\n",
    "            gradient_clip_val=config.GRADIENT_CLIP_VAL,\n",
    "            callbacks=[lr_logger, early_stop, checkpoint, print_metrics],\n",
    "            limit_train_batches=config.LIMIT_TRAIN_BATCHES,\n",
    "            val_check_interval=config.VAL_CHECK_INTERVAL,\n",
    "            log_every_n_steps=config.LOG_EVERY_N_STEPS,\n",
    "            enable_progress_bar=True,\n",
    "        )\n",
    "\n",
    "        # Build multivariate loss\n",
    "        multivar_loss = MultiLoss(\n",
    "            [NormalDistributionLoss() for _ in config.TARGETS]\n",
    "        )\n",
    "\n",
    "        # Instantiate model\n",
    "        model = DeepAR.from_dataset(\n",
    "            train_ds,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            hidden_size=config.DEEPAR_CONFIG[\"lstm_hidden_dim\"],\n",
    "            rnn_layers=config.DEEPAR_CONFIG[\"lstm_layers\"],\n",
    "            dropout=config.DEEPAR_CONFIG[\"lstm_dropout\"],\n",
    "            optimizer=config.OPTIMIZER,\n",
    "            loss=multivar_loss,\n",
    "        )\n",
    "\n",
    "        print(f\"Training {model_name}…\")\n",
    "        trainer.fit(model, train_loader, test_loader)\n",
    "\n",
    "        # Load best checkpoint\n",
    "        best_model_path = checkpoint.best_model_path\n",
    "        exp_manager.log_artifact(best_model_path, \"model\")\n",
    "        best_model = DeepAR.load_from_checkpoint(best_model_path)\n",
    "\n",
    "        print(\"Evaluating model…\")\n",
    "        predictions = best_model.predict(\n",
    "            test_loader,\n",
    "            trainer_kwargs=dict(accelerator=\"auto\"),\n",
    "            batch_size=1\n",
    "        )\n",
    "\n",
    "        # Calculate and log metrics\n",
    "        metrics = {\n",
    "            'MAE': MAE(),\n",
    "            'RMSE': RMSE(),\n",
    "            'SMAPE': SMAPE(),\n",
    "            'MAPE': MAPE()\n",
    "        }\n",
    "        eval_metrics = training.model_evaluation(predictions, metrics)\n",
    "        exp_manager.log_metrics(eval_metrics)\n",
    "        plotting.model_plotting_function(best_model,test_loader)\n",
    "\n",
    "        print(f\"✅ {model_name} training complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {model_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        exp_manager.end_mlflow_run()\n",
    "\n",
    "    return run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_deepar_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-28 01:42:02.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.mlflow_utils\u001b[0m:\u001b[36m_set_tracking_uri\u001b[0m:\u001b[36m55\u001b[0m - \u001b[1mMLflow tracking URI set to: file:///D:%5CCML%5CTerm%208%5CML%20projects%5Cforecasting_workspace%5Coceanwave_forecast%5Cmlruns\u001b[0m\n",
      "\u001b[32m2025-07-28 01:42:02.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.mlflow_utils\u001b[0m:\u001b[36mget_or_create_experiment\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mFound existing experiment 'Oceanwave_DeepAR_Training' with ID 828880404737217571.\u001b[0m\n",
      "\u001b[32m2025-07-28 01:42:02.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.mlflow_utils\u001b[0m:\u001b[36mstart_mlflow_run\u001b[0m:\u001b[36m134\u001b[0m - \u001b[1mStarted MLflow run with ID: 57f8ee64fef04f62b8090d1c05ea5e31\u001b[0m\n",
      "\u001b[32m2025-07-28 01:42:02.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.mlflow_utils\u001b[0m:\u001b[36mlog_params\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1mLogged parameters: dict_keys(['lstm_hidden_dim', 'lstm_layers', 'lstm_dropout', 'embedding_dim', 'num_class'])\u001b[0m\n",
      "\u001b[32m2025-07-28 01:42:02.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.mlflow_utils\u001b[0m:\u001b[36mlog_param\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mLogged parameter: model_type=DeepAR\u001b[0m\n",
      "\u001b[32m2025-07-28 01:42:02.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.mlflow_utils\u001b[0m:\u001b[36mlog_param\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mLogged parameter: targets=WVHT,APD\u001b[0m\n",
      "\u001b[32m2025-07-28 01:42:02.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.mlflow_utils\u001b[0m:\u001b[36mlog_param\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mLogged parameter: batch_size=64\u001b[0m\n",
      "\u001b[32m2025-07-28 01:42:02.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.mlflow_utils\u001b[0m:\u001b[36mlog_param\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mLogged parameter: max_epochs=2500\u001b[0m\n",
      "\u001b[32m2025-07-28 01:42:02.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.mlflow_utils\u001b[0m:\u001b[36mlog_param\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mLogged parameter: learning_rate=0.001\u001b[0m\n",
      "\u001b[32m2025-07-28 01:42:02.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.mlflow_utils\u001b[0m:\u001b[36mlog_param\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mLogged parameter: gradient_clip_val=0.1\u001b[0m\n",
      "\u001b[32m2025-07-28 01:42:02.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.mlflow_utils\u001b[0m:\u001b[36mlog_param\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mLogged parameter: optimizer=adamw\u001b[0m\n",
      "\u001b[32m2025-07-28 01:42:02.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.mlflow_utils\u001b[0m:\u001b[36mlog_param\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mLogged parameter: window=504\u001b[0m\n",
      "\u001b[32m2025-07-28 01:42:02.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.mlflow_utils\u001b[0m:\u001b[36mlog_param\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mLogged parameter: horizon=72\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:197: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:197: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type           | Params\n",
      "----------------------------------------------------------\n",
      "0 | loss                   | MultiLoss      | 0     \n",
      "1 | logging_metrics        | ModuleList     | 0     \n",
      "2 | embeddings             | MultiEmbedding | 0     \n",
      "3 | rnn                    | LSTM           | 351 K \n",
      "4 | distribution_projector | ModuleList     | 516   \n",
      "----------------------------------------------------------\n",
      "351 K     Trainable params\n",
      "0         Non-trainable params\n",
      "351 K     Total params\n",
      "1.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DeepAR…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9f98d83ede462d82561f50bb6e332e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Summary:\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e404733a31452ba0227e8a61564ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9eae81da5634b27a4cf282145327928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Summary:\n",
      "  -> Validation Loss: 2.4937\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ee2f62b6e14427850199e29355115e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Summary:\n",
      "  -> Validation Loss: 1.6841\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de72048d3814374814d8b4bb81f362e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a8a2e9a0164f20ba76ed23159d390a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f74662764864246a9e388cf351b026f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709afa93673743afbfd6844c1761dc40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739f7a50a1c84bc99ee36bd7b882492e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1b7bdecd2545e5a082c964fd088f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4358632325fb4b558cc2f8ee11e7aec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a52b11dbac54869be32abc33b0374cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1362a9f5567046abb67fd23c7eca82d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98947dc21ba48398a1c89828a03cbf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ac205f0cd84c9190435174765e8b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24caa07cd3164f29bec0e4c4d6a85b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abd24f37e874add9d2d9ba1cd28e338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad2ad832c6249109a9fdacd2fab444e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f487db3456db48379054c13fb29faebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73e58ada3334528ab6d2e5881480736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c548cf771ef4da893b9094bcd91ca67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a204434eef3e44afbe02bc76aed6eb88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324bc7762a774f44a639322386bcf7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Summary:\n",
      "  -> Train Loss: -0.6771\n",
      "  -> Validation Loss: 0.6504\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac0d597b7e5436cbed5eea930d29dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-28 01:43:06.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.mlflow_utils\u001b[0m:\u001b[36mlog_artifact\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mLogged artifact: d:\\CML\\Term 8\\ML projects\\forecasting_workspace\\oceanwave_forecast\\notebooks\\lightning_logs\\version_17\\checkpoints\\DeepAR-epoch=05-val_loss=val_loss=0.3238.ckpt to model\u001b[0m\n",
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:197: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:197: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Summary:\n",
      "  -> Train Loss: -0.6771\n",
      "  -> Validation Loss: 0.6619\n",
      "------------------------------\n",
      "Evaluating model…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\u001b[32m2025-07-28 01:43:08.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.mlflow_utils\u001b[0m:\u001b[36mend_mlflow_run\u001b[0m:\u001b[36m147\u001b[0m - \u001b[1mEnded MLflow run with ID: 57f8ee64fef04f62b8090d1c05ea5e31\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DeepAR training complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_deepar_training(\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    train_ds=train_ds,\n",
    "    run_number=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457d615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
