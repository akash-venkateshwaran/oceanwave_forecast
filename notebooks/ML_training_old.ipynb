{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3dcc529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-27 10:50:45.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mPROJ_ROOT path is: D:\\CML\\Term 8\\ML projects\\forecasting_workspace\\oceanwave_forecast\u001b[0m\n",
      "\u001b[32m2025-07-27 10:50:45.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1mPROJ_ROOT path is: D:\\CML\\Term 8\\ML projects\\forecasting_workspace\\oceanwave_forecast\u001b[0m\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ocean Wave Height and Period Forecasting with DeepAR\n",
    "# Deep Autoregressive Time Series Modeling using PyTorch Forecasting\n",
    "\n",
    "import warnings, numpy as np, pandas as pd, torch\n",
    "import matplotlib.pyplot as plt\n",
    "import lightning as pl\n",
    "import pytorch_forecasting as ptf\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from sktime.split import temporal_train_test_split\n",
    "import importlib\n",
    "\n",
    "from oceanwave_forecast import data_manager, data_pipeline, forecasting_utils, config, mlflow_utils, training\n",
    "\n",
    "importlib.reload(data_manager)\n",
    "importlib.reload(data_pipeline)\n",
    "importlib.reload(forecasting_utils)\n",
    "importlib.reload(config)\n",
    "importlib.reload(mlflow_utils)\n",
    "importlib.reload(training)\n",
    "\n",
    "from collections import namedtuple\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer, MultiNormalizer\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "pl.seed_everything(config.RANDOM_STATE)\n",
    "torch.manual_seed(config.RANDOM_STATE)\n",
    "np.random.seed(config.RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fab36c",
   "metadata": {},
   "source": [
    "# 1. DATA PREPARATION AND PREPROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94333a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureConfig = namedtuple(\n",
    "    \"FeatureConfig\",\n",
    "    [\n",
    "        \"target\",\n",
    "        \"index_cols\",\n",
    "        \"static_categoricals\",\n",
    "        \"static_reals\",\n",
    "        \"time_varying_known_categoricals\",\n",
    "        \"time_varying_known_reals\",\n",
    "        \"time_varying_unknown_reals\",\n",
    "        \"group_ids\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "feat_cfg = FeatureConfig(\n",
    "    target              = \"Hs\",                              # <- main forecast target\n",
    "    index_cols          = [\"series\", \"timestamp\"],           # timestamp + series ID\n",
    "    static_categoricals = [\"series\"],                        # ocean buoy ID\n",
    "    static_reals        = [],\n",
    "    time_varying_known_categoricals = [],                    # e.g. holiday flags\n",
    "    time_varying_known_reals        = [\"time_idx\"],          # we always know time\n",
    "    time_varying_unknown_reals      = [],                    # filled later (lags & exog)\n",
    "    group_ids           = [\"series\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce99db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\CML\\Term 8\\ML projects\\forecasting_workspace\\oceanwave_forecast\\oceanwave_forecast\\data_manager.py:43: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pd.read_csv(\n",
      "D:\\CML\\Term 8\\ML projects\\forecasting_workspace\\oceanwave_forecast\\oceanwave_forecast\\data_pipeline.py:111: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  data_ocean_hourly = data_ocean_clean.resample('H').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (52650, 13)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 52650 entries, 2024-01-01 00:00:00 to 2024-12-31 23:50:00\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   WDIR    52650 non-null  float64\n",
      " 1   WSPD    52650 non-null  float64\n",
      " 2   GST     52650 non-null  float64\n",
      " 3   WVHT    52650 non-null  float64\n",
      " 4   DPD     52650 non-null  float64\n",
      " 5   APD     52650 non-null  float64\n",
      " 6   MWD     52650 non-null  float64\n",
      " 7   PRES    52650 non-null  float64\n",
      " 8   ATMP    52650 non-null  float64\n",
      " 9   WTMP    52650 non-null  float64\n",
      " 10  DEWP    52650 non-null  float64\n",
      " 11  VIS     52650 non-null  float64\n",
      " 12  TIDE    52650 non-null  float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 5.6 MB\n",
      "\n",
      "Descriptive statistics:\n",
      "               WDIR          WSPD           GST          WVHT           DPD  \\\n",
      "count  52650.000000  52650.000000  52650.000000  52650.000000  52650.000000   \n",
      "mean     194.421026      4.962283      6.241216     66.211796     80.537227   \n",
      "std       96.600677      3.805890      4.484900     46.447783     37.382029   \n",
      "min        1.000000      0.000000      0.000000      0.030000      2.060000   \n",
      "25%      119.000000      2.300000      3.000000      0.550000     99.000000   \n",
      "50%      225.000000      4.300000      5.300000     99.000000     99.000000   \n",
      "75%      260.000000      6.900000      8.500000     99.000000     99.000000   \n",
      "max      999.000000     99.000000     99.000000     99.000000     99.000000   \n",
      "\n",
      "                APD           MWD          PRES          ATMP          WTMP  \\\n",
      "count  52650.000000  52650.000000  52650.000000  52650.000000  52650.000000   \n",
      "mean      67.337013    844.439943   1016.577221     10.072936     14.923626   \n",
      "std       44.857096    314.352896     78.656110     21.306571     71.185836   \n",
      "min        2.270000      0.000000    985.000000     -9.800000      8.000000   \n",
      "25%        4.230000    999.000000   1012.000000      8.100000      8.900000   \n",
      "50%       99.000000    999.000000   1016.600000      9.700000      9.600000   \n",
      "75%       99.000000    999.000000   1020.900000     11.500000     10.600000   \n",
      "max       99.000000    999.000000   9999.000000    999.000000    999.000000   \n",
      "\n",
      "               DEWP      VIS     TIDE  \n",
      "count  52650.000000  52650.0  52650.0  \n",
      "mean       7.827449     99.0     99.0  \n",
      "std       21.475054      0.0      0.0  \n",
      "min      -16.100000     99.0     99.0  \n",
      "25%        5.300000     99.0     99.0  \n",
      "50%        7.800000     99.0     99.0  \n",
      "75%       10.100000     99.0     99.0  \n",
      "max      999.000000     99.0     99.0  \n"
     ]
    }
   ],
   "source": [
    "raw_path   = config.RAW_DATA_DIR / \"Standard meteorological data 2024\" / \"46088h2024.txt\"\n",
    "df_raw     = data_manager.extract_raw_data(raw_path)\n",
    "df_clean   = data_pipeline.preprocess_ocean_data(df_raw)\n",
    "# df_clean   = df_clean.loc[config.START_DATE : config.END_DATE]\n",
    "\n",
    "# split target & features\n",
    "Y = df_clean[config.TARGETS]\n",
    "X = df_clean.drop(columns=config.TARGETS)\n",
    "\n",
    "y_train, y_test, X_train, X_test = temporal_train_test_split(\n",
    "    y=Y, X=X, test_size=config.HORIZON * 3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102ba87",
   "metadata": {},
   "source": [
    "# 2. FEATURE ENGINEERING FOR DEEPAR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5677f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\pmdarima\\arima\\_validation.py:76: UserWarning: stepwise model cannot be fit in parallel (n_jobs=1). Falling back to stepwise parameter search.\n",
      "  warnings.warn('stepwise model cannot be fit in parallel (n_jobs=%i). '\n",
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\pmdarima\\arima\\_validation.py:76: UserWarning: stepwise model cannot be fit in parallel (n_jobs=1). Falling back to stepwise parameter search.\n",
      "  warnings.warn('stepwise model cannot be fit in parallel (n_jobs=%i). '\n",
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\pmdarima\\arima\\_validation.py:76: UserWarning: stepwise model cannot be fit in parallel (n_jobs=1). Falling back to stepwise parameter search.\n",
      "  warnings.warn('stepwise model cannot be fit in parallel (n_jobs=%i). '\n"
     ]
    }
   ],
   "source": [
    "pipe_X, pipe_Y = data_pipeline.get_pipelines(list(X_train.columns))\n",
    "\n",
    "X_train_transformed = pipe_X.fit_transform(X_train)\n",
    "X_test_transformed  = pipe_X.transform(X_test)\n",
    "y_train_transformed = pipe_Y.fit_transform(y_train)\n",
    "y_test_transformed  = pipe_Y.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "288d79aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WVHT</th>\n",
       "      <th>APD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-01-01 00:00:00</th>\n",
       "      <td>-0.757493</td>\n",
       "      <td>0.779594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-01 01:00:00</th>\n",
       "      <td>-0.698073</td>\n",
       "      <td>1.456019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-01 02:00:00</th>\n",
       "      <td>-0.772348</td>\n",
       "      <td>1.486535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-01 03:00:00</th>\n",
       "      <td>-0.816913</td>\n",
       "      <td>2.188389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-01 04:00:00</th>\n",
       "      <td>-0.831767</td>\n",
       "      <td>2.595261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-22 19:00:00</th>\n",
       "      <td>4.486290</td>\n",
       "      <td>0.932171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-22 20:00:00</th>\n",
       "      <td>4.426871</td>\n",
       "      <td>0.932171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-22 21:00:00</th>\n",
       "      <td>4.233757</td>\n",
       "      <td>0.916914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-22 22:00:00</th>\n",
       "      <td>4.189192</td>\n",
       "      <td>0.774508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-22 23:00:00</th>\n",
       "      <td>3.535576</td>\n",
       "      <td>0.627017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8568 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         WVHT       APD\n",
       "datetime                               \n",
       "2024-01-01 00:00:00 -0.757493  0.779594\n",
       "2024-01-01 01:00:00 -0.698073  1.456019\n",
       "2024-01-01 02:00:00 -0.772348  1.486535\n",
       "2024-01-01 03:00:00 -0.816913  2.188389\n",
       "2024-01-01 04:00:00 -0.831767  2.595261\n",
       "...                       ...       ...\n",
       "2024-12-22 19:00:00  4.486290  0.932171\n",
       "2024-12-22 20:00:00  4.426871  0.932171\n",
       "2024-12-22 21:00:00  4.233757  0.916914\n",
       "2024-12-22 22:00:00  4.189192  0.774508\n",
       "2024-12-22 23:00:00  3.535576  0.627017\n",
       "\n",
       "[8568 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57ba8718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset shape: (8784, 15)\n",
      "Time index range: 0 to 8783\n"
     ]
    }
   ],
   "source": [
    "# Note that none of the dataframes X or y has timeindex cols. it is datatimeindexed but htere is not dedicated column for it.\n",
    "\n",
    "\n",
    "def tidy_long_df(X, y, buoy_id=\"buoy_46088\"):\n",
    "    \"\"\"\n",
    "    Merge features + targets and give PyTorchâ€‘Forecasting its three required columns:\n",
    "      â€¢ series     â€“ constant 'buoy_46088'\n",
    "      â€¢ time_idx   â€“ 0â€¦N counter per series (int64, contiguous)\n",
    "      â€¢ target     â€“ numeric columns to predict\n",
    "    \"\"\"\n",
    "    # 1) concat horizontally, then pull the datetime index into a column\n",
    "    df = pd.concat([X, y], axis=1).reset_index()  \n",
    "    # 2) rename the datetime column\n",
    "    df.rename(columns={\"datetime\": \"timestamp\"}, inplace=True)\n",
    "    # 3) assign the single-series ID\n",
    "    df[\"series\"]   = buoy_id\n",
    "    # 4) zeroâ€‘based time index\n",
    "    df[\"time_idx\"] = np.arange(len(df), dtype=np.int64)\n",
    "    return df\n",
    "\n",
    "# build your full_df with vertical concat on X and y, then tidy:\n",
    "full_df = tidy_long_df(\n",
    "    pd.concat([X_train_transformed, X_test_transformed]),\n",
    "    pd.concat([y_train_transformed, y_test_transformed])\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Full dataset shape: {full_df.shape}\")\n",
    "print(f\"Time index range: {full_df.time_idx.min()} to {full_df.time_idx.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75fe447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "  Encoder length: 504\n",
      "  Prediction length: 72\n",
      "  Minimum sequence length: 576\n",
      "  Total data length: 8784\n",
      "\n",
      "Split boundaries:\n",
      "  Train: 0 to 7631\n",
      "  Val: 7632 to 8207\n",
      "  Test: 8208 to 8783\n",
      "\n",
      "Actual split shapes:\n",
      "  Train: (7632, 15)\n",
      "  Val: (576, 15)\n",
      "  Test: (576, 15)\n",
      "\n",
      "Validating splits:\n",
      "âœ… Train split: 7632 samples (sufficient for 576 minimum)\n",
      "âœ… Val split: 576 samples (sufficient for 576 minimum)\n",
      "âœ… Test split: 576 samples (sufficient for 576 minimum)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_pred_len   = config.HORIZON        # 72 hours\n",
    "enc_len        = config.WINDOW         # 504 hours  \n",
    "n_test_steps   = max_pred_len * 3      # 216 hours\n",
    "\n",
    "# FIXED SPLITTING LOGIC\n",
    "# We need to ensure each split has enough data to create valid sequences\n",
    "total_length = len(full_df)\n",
    "min_sequence_length = enc_len + max_pred_len  # 576 hours\n",
    "\n",
    "print(f\"Parameters:\")\n",
    "print(f\"  Encoder length: {enc_len}\")\n",
    "print(f\"  Prediction length: {max_pred_len}\")\n",
    "print(f\"  Minimum sequence length: {min_sequence_length}\")\n",
    "print(f\"  Total data length: {total_length}\")\n",
    "\n",
    "# Reserve test data - needs encoder + prediction length\n",
    "test_size = max(n_test_steps, min_sequence_length)  # At least 576 hours\n",
    "test_start = total_length - test_size\n",
    "\n",
    "# Reserve validation data - needs encoder + prediction length  \n",
    "val_size = min_sequence_length  # 576 hours minimum\n",
    "val_start = test_start - val_size\n",
    "\n",
    "print(f\"\\nSplit boundaries:\")\n",
    "print(f\"  Train: 0 to {val_start-1}\")\n",
    "print(f\"  Val: {val_start} to {test_start-1}\")  \n",
    "print(f\"  Test: {test_start} to {total_length-1}\")\n",
    "\n",
    "# Create splits with proper sizes\n",
    "train_df = full_df[full_df.time_idx < val_start]\n",
    "val_df = full_df[(full_df.time_idx >= val_start) & (full_df.time_idx < test_start)]\n",
    "test_df = full_df[full_df.time_idx >= test_start]\n",
    "\n",
    "print(f\"\\nActual split shapes:\")\n",
    "print(f\"  Train: {train_df.shape}\")\n",
    "print(f\"  Val: {val_df.shape}\")  \n",
    "print(f\"  Test: {test_df.shape}\")\n",
    "\n",
    "# Verify splits can create valid sequences\n",
    "def check_split_validity(df, split_name, enc_len, pred_len):\n",
    "    if len(df) == 0:\n",
    "        print(f\"ERROR: {split_name} split is empty!\")\n",
    "        return False\n",
    "    \n",
    "    min_needed = enc_len + pred_len\n",
    "    if len(df) < min_needed:\n",
    "        print(f\"ERROR: {split_name} split too small! Has {len(df)} samples, needs at least {min_needed}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"âœ… {split_name} split: {len(df)} samples (sufficient for {min_needed} minimum)\")\n",
    "    return True\n",
    "\n",
    "print(f\"\\nValidating splits:\")\n",
    "train_valid = check_split_validity(train_df, \"Train\", enc_len, max_pred_len)\n",
    "val_valid = check_split_validity(val_df, \"Val\", enc_len, max_pred_len)\n",
    "test_valid = check_split_validity(test_df, \"Test\", enc_len, max_pred_len)\n",
    "\n",
    "if not all([train_valid, val_valid, test_valid]):\n",
    "    print(\"\\nâŒ Invalid splits detected!\")\n",
    "    # Fallback: simple percentage splits\n",
    "    print(\"Falling back to percentage-based splits...\")\n",
    "    \n",
    "    # Use 70-15-15 split but ensure minimum sizes\n",
    "    train_end = int(0.7 * total_length)\n",
    "    val_end = min(train_end + max(int(0.15 * total_length), min_sequence_length), \n",
    "                  total_length - min_sequence_length)\n",
    "    \n",
    "    train_df = full_df[:train_end]\n",
    "    val_df = full_df[train_end:val_end]  \n",
    "    test_df = full_df[val_end:]\n",
    "    \n",
    "    print(f\"Fallback splits:\")\n",
    "    print(f\"  Train: {train_df.shape}\")\n",
    "    print(f\"  Val: {val_df.shape}\")\n",
    "    print(f\"  Test: {test_df.shape}\")\n",
    "    \n",
    "    # Re-validate\n",
    "    train_valid = check_split_validity(train_df, \"Train\", enc_len, max_pred_len)\n",
    "    val_valid = check_split_validity(val_df, \"Val\", enc_len, max_pred_len)\n",
    "    test_valid = check_split_validity(test_df, \"Test\", enc_len, max_pred_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5966bb2",
   "metadata": {},
   "source": [
    "# 3. TIMESERIESDATASET CONFIGURATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bb503af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All splits valid! Proceeding with dataset creation...\n",
      "Features (time_varying_unknown_reals): ['WSPD', 'GST', 'PRES', 'ATMP', 'WTMP', 'DEWP', 'WDIR_sin', 'WDIR_cos', 'MWD_sin', 'MWD_cos']\n",
      "Targets: ['WVHT', 'APD']\n",
      "\n",
      "Creating training dataset...\n",
      "âœ… Training dataset created: 7703 sequences\n",
      "Creating validation dataset...\n",
      "âœ… Validation dataset created: 647 sequences\n",
      "Creating test dataset...\n",
      "âœ… Test dataset created: 1 sequences\n",
      "\n",
      "ðŸŽ‰ All datasets created successfully!\n",
      "Summary:\n",
      "  - Training sequences: 7703\n",
      "  - Validation sequences: 647\n",
      "  - Test sequences: 1\n"
     ]
    }
   ],
   "source": [
    "# If we have valid splits, proceed with dataset creation\n",
    "if all([train_valid, val_valid, test_valid]):\n",
    "        print(f\"\\nâœ… All splits valid! Proceeding with dataset creation...\")\n",
    "        \n",
    "        # Ensure target columns are properly configured\n",
    "        target_cols = config.TARGETS if isinstance(config.TARGETS, list) else [config.TARGETS]\n",
    "        feature_cols = [c for c in X_train_transformed.columns]\n",
    "        \n",
    "        # All features go into time_varying_unknown_reals\n",
    "        # Targets are automatically handled by PyTorch Forecasting\n",
    "        time_varying_unknown_reals = feature_cols\n",
    "        \n",
    "        print(f\"Features (time_varying_unknown_reals): {time_varying_unknown_reals}\")\n",
    "        print(f\"Targets: {target_cols}\")\n",
    "        \n",
    "        # build one GroupNormalizer *per* target\n",
    "        normalizers = [\n",
    "            GroupNormalizer(\n",
    "                groups=[\"series\"],\n",
    "                transformation=\"softplus\"\n",
    "            )\n",
    "            for _ in target_cols\n",
    "        ]\n",
    "\n",
    "        common = dict(\n",
    "            time_idx                   = \"time_idx\",\n",
    "            target                     = config.TARGETS,\n",
    "            group_ids                  = [\"series\"],\n",
    "            time_varying_known_reals   = [\"time_idx\"],  # Only time_idx is known in future\n",
    "            time_varying_unknown_reals = time_varying_unknown_reals,  # Features only\n",
    "            static_categoricals        = [\"series\"],\n",
    "            max_encoder_length         = enc_len,\n",
    "            max_prediction_length      = max_pred_len,\n",
    "            min_encoder_length         = max(enc_len // 2, 1),  # More flexible minimum\n",
    "            min_prediction_length      = 1,\n",
    "            target_normalizer          = MultiNormalizer(normalizers),\n",
    "            allow_missing_timesteps    = True,\n",
    "        )\n",
    "    \n",
    "    \n",
    "        print(\"\\nCreating training dataset...\")\n",
    "        train_ds = TimeSeriesDataSet(train_df, **common)\n",
    "        print(f\"âœ… Training dataset created: {len(train_ds)} sequences\")\n",
    "        \n",
    "        print(\"Creating validation dataset...\")\n",
    "        val_ds = TimeSeriesDataSet.from_dataset(train_ds, val_df, stop_randomization=True)\n",
    "        print(f\"âœ… Validation dataset created: {len(val_ds)} sequences\")\n",
    "        \n",
    "        print(\"Creating test dataset...\")\n",
    "        test_ds = TimeSeriesDataSet.from_dataset(\n",
    "            train_ds, test_df,\n",
    "            predict=True, stop_randomization=True\n",
    "        )\n",
    "        print(f\"âœ… Test dataset created: {len(test_ds)} sequences\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ All datasets created successfully!\")\n",
    "        print(f\"Summary:\")\n",
    "        print(f\"  - Training sequences: {len(train_ds)}\")\n",
    "        print(f\"  - Validation sequences: {len(val_ds)}\")\n",
    "        print(f\"  - Test sequences: {len(test_ds)}\")\n",
    "        \n",
    "        \n",
    " \n",
    "\n",
    "else:\n",
    "    print(\"âŒ Cannot create datasets - invalid splits!\")\n",
    "    print(\"Consider:\")\n",
    "    print(\"1. Reducing WINDOW (encoder length) - try ONE_WEEK * 2 instead of 3\")  \n",
    "    print(\"2. Reducing HORIZON (prediction length) - try ONE_DAY * 2 instead of 3\")\n",
    "    print(\"3. Getting more training data\")\n",
    "    \n",
    "    # Show what would work\n",
    "    max_possible_enc = (total_length - 2 * max_pred_len) // 2\n",
    "    print(f\"4. Maximum encoder length that would work: {max_possible_enc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc2d92",
   "metadata": {},
   "source": [
    "# 4. TRAINING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfb2db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch   = config.DEEPAR_CONFIG[\"batch_size\"]\n",
    "\n",
    "train_loader = train_ds.to_dataloader(train=True,  batch_size=batch, num_workers=4)\n",
    "val_loader   = val_ds  .to_dataloader(train=False, batch_size=batch)\n",
    "test_loader  = test_ds .to_dataloader(train=False, batch_size=batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fb17aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_cat: shape torch.Size([64, 504, 1])\n",
      "encoder_cont: shape torch.Size([64, 504, 12])\n",
      "encoder_target: [tensor([[ 0.5349,  0.3715,  0.2081,  ..., -0.8912, -0.9060, -0.9209],\n",
      "        [ 0.3715,  0.2081,  0.0892,  ..., -0.9060, -0.9209,  0.0000],\n",
      "        [ 0.2081,  0.0892, -0.1187,  ..., -0.9209,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.3566,  0.9954,  1.6639,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9954,  1.6639,  2.0798,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.6639,  2.0798,  1.7233,  ...,  0.0000,  0.0000,  0.0000]]), tensor([[-0.4359, -0.4614, -0.6292,  ...,  0.4134,  0.8152,  1.5577],\n",
      "        [-0.4614, -0.6292, -0.5224,  ...,  0.8152,  1.5577,  0.0000],\n",
      "        [-0.6292, -0.5224, -0.3291,  ...,  1.5577,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.1766,  0.1439,  0.1998,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1439,  0.1998,  0.1693,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1998,  0.1693, -0.0952,  ...,  0.0000,  0.0000,  0.0000]])]\n",
      "encoder_lengths: shape torch.Size([64])\n",
      "decoder_cat: shape torch.Size([64, 72, 1])\n",
      "decoder_cont: shape torch.Size([64, 72, 12])\n",
      "decoder_target: [tensor([[-0.8763, -0.8021, -0.6238,  ...,  1.4410,  2.1244,  3.0899],\n",
      "        [-0.8763, -0.8021, -0.6238,  ...,  1.4410,  2.1244,  3.0899],\n",
      "        [-0.8763, -0.8021, -0.6238,  ...,  1.4410,  2.1244,  3.0899],\n",
      "        ...,\n",
      "        [-0.8763, -0.8021, -0.6238,  ...,  1.4410,  2.1244,  3.0899],\n",
      "        [-0.8763, -0.8021, -0.6238,  ...,  1.4410,  2.1244,  3.0899],\n",
      "        [-0.8763, -0.8021, -0.6238,  ...,  1.4410,  2.1244,  3.0899]]), tensor([[ 0.0421, -0.7462, -1.0259,  ...,  0.2303,  0.6270,  0.7796],\n",
      "        [ 0.0421, -0.7462, -1.0259,  ...,  0.2303,  0.6270,  0.7796],\n",
      "        [ 0.0421, -0.7462, -1.0259,  ...,  0.2303,  0.6270,  0.7796],\n",
      "        ...,\n",
      "        [ 0.0421, -0.7462, -1.0259,  ...,  0.2303,  0.6270,  0.7796],\n",
      "        [ 0.0421, -0.7462, -1.0259,  ...,  0.2303,  0.6270,  0.7796],\n",
      "        [ 0.0421, -0.7462, -1.0259,  ...,  0.2303,  0.6270,  0.7796]])]\n",
      "decoder_lengths: shape torch.Size([64])\n",
      "decoder_time_idx: shape torch.Size([64, 72])\n",
      "groups: shape torch.Size([64, 1])\n",
      "target_scale: [tensor([[-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216],\n",
      "        [-0.1712,  2.0216]]), tensor([[-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950],\n",
      "        [-0.1770,  1.7950]])]\n"
     ]
    }
   ],
   "source": [
    "# Get one batch from val_loader\n",
    "sample_batch = next(iter(val_loader))\n",
    "# If sample_batch is a tuple, extract the first element (assumed to be the dictionary with batch data)\n",
    "if isinstance(sample_batch, tuple):\n",
    "    sample_batch = sample_batch[0]\n",
    "\n",
    "# Print keys and shapes for tensors in the batch\n",
    "for key, value in sample_batch.items():\n",
    "    if torch.is_tensor(value):\n",
    "        print(f\"{key}: shape {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "240ea7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akashv22\\AppData\\Local\\anaconda3\\envs\\fc_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "\u001b[32m2025-07-21 13:55:51.092\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m386\u001b[0m - \u001b[1mInitialized DeepAR trainer on device: cuda\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer = training.DeepARTrainer(config.DEEPAR_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fee62b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-21 13:55:51.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m559\u001b[0m - \u001b[1mStarting synthetic training for 2000 epochs\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 1/2000 | Train Loss: 0.1515 | Val Loss: 0.1673 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.110\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 2/2000 | Train Loss: 0.1444 | Val Loss: 0.1656 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 3/2000 | Train Loss: 0.1530 | Val Loss: 0.1719 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 4/2000 | Train Loss: 0.1536 | Val Loss: 0.1704 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.111\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 5/2000 | Train Loss: 0.1388 | Val Loss: 0.1601 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 6/2000 | Train Loss: 0.1417 | Val Loss: 0.1749 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 7/2000 | Train Loss: 0.1485 | Val Loss: 0.1673 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 8/2000 | Train Loss: 0.1459 | Val Loss: 0.1608 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 9/2000 | Train Loss: 0.1470 | Val Loss: 0.1743 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 10/2000 | Train Loss: 0.1425 | Val Loss: 0.1705 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.114\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 11/2000 | Train Loss: 0.1508 | Val Loss: 0.1588 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 12/2000 | Train Loss: 0.1500 | Val Loss: 0.1767 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.115\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 13/2000 | Train Loss: 0.1460 | Val Loss: 0.1697 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 14/2000 | Train Loss: 0.1510 | Val Loss: 0.1653 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 15/2000 | Train Loss: 0.1473 | Val Loss: 0.1746 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 16/2000 | Train Loss: 0.1404 | Val Loss: 0.1656 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 17/2000 | Train Loss: 0.1462 | Val Loss: 0.1641 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 18/2000 | Train Loss: 0.1392 | Val Loss: 0.1600 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 19/2000 | Train Loss: 0.1480 | Val Loss: 0.1671 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 20/2000 | Train Loss: 0.1430 | Val Loss: 0.1661 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 21/2000 | Train Loss: 0.1420 | Val Loss: 0.1630 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 22/2000 | Train Loss: 0.1392 | Val Loss: 0.1578 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 23/2000 | Train Loss: 0.1483 | Val Loss: 0.1630 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 24/2000 | Train Loss: 0.1411 | Val Loss: 0.1615 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 25/2000 | Train Loss: 0.1394 | Val Loss: 0.1536 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 26/2000 | Train Loss: 0.1394 | Val Loss: 0.1524 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.122\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 27/2000 | Train Loss: 0.1435 | Val Loss: 0.1643 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 28/2000 | Train Loss: 0.1424 | Val Loss: 0.1449 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 29/2000 | Train Loss: 0.1423 | Val Loss: 0.1614 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.123\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 30/2000 | Train Loss: 0.1420 | Val Loss: 0.1570 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 31/2000 | Train Loss: 0.1502 | Val Loss: 0.1674 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 32/2000 | Train Loss: 0.1371 | Val Loss: 0.1562 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 33/2000 | Train Loss: 0.1363 | Val Loss: 0.1628 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 34/2000 | Train Loss: 0.1344 | Val Loss: 0.1561 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.125\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 35/2000 | Train Loss: 0.1413 | Val Loss: 0.1589 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 36/2000 | Train Loss: 0.1435 | Val Loss: 0.1623 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 37/2000 | Train Loss: 0.1369 | Val Loss: 0.1547 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 38/2000 | Train Loss: 0.1330 | Val Loss: 0.1632 | LR: 1.00e-03\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.127\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 39/2000 | Train Loss: 0.1327 | Val Loss: 0.1509 | LR: 5.00e-04\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 40/2000 | Train Loss: 0.1398 | Val Loss: 0.1583 | LR: 5.00e-04\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 41/2000 | Train Loss: 0.1399 | Val Loss: 0.1617 | LR: 5.00e-04\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mEpoch 42/2000 | Train Loss: 0.1386 | Val Loss: 0.1598 | LR: 5.00e-04\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.128\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m584\u001b[0m - \u001b[1mEarly stopping at epoch 43\u001b[0m\n",
      "\u001b[32m2025-07-21 13:55:51.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36moceanwave_forecast.training\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m594\u001b[0m - \u001b[1mSynthetic training completed!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "history = trainer.train(train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bd2f46",
   "metadata": {},
   "source": [
    "# 5. MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5114c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Processing 3 blocks of size 72\n",
      "  Block 1 MeanSquaredPercentageError: 0.2306\n",
      "  Block 1 MeanAbsolutePercentageError: 0.1674\n",
      "  Block 2 MeanSquaredPercentageError: 0.1683\n",
      "  Block 2 MeanAbsolutePercentageError: 0.1358\n",
      "  Block 3 MeanSquaredPercentageError: 0.2291\n",
      "  Block 3 MeanAbsolutePercentageError: 0.1742\n",
      "\n",
      "ðŸ“Š Aggregated Scores:\n",
      "  avg_MeanSquaredPercentageError: 0.2093\n",
      "  std_MeanSquaredPercentageError: 0.0290\n",
      "  avg_MeanAbsolutePercentageError: 0.1591\n",
      "  std_MeanAbsolutePercentageError: 0.0167\n",
      "âœ… DeepAR predict complete (3 blocks)\n"
     ]
    }
   ],
   "source": [
    "results = trainer.predict(y_train, y_test, config.SCORERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d71ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8628746b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
